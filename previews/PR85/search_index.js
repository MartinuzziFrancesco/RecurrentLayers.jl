var documenterSearchIndex = {"docs":
[{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"RANCell\nIndRNNCell\nLightRUCell\nLiGRUCell\nMGUCell\nNASCell\nRHNCell\nRHNCellUnit\nMUT1Cell\nMUT2Cell\nMUT3Cell\nSCRNCell\nPeepholeLSTMCell\nFastRNNCell\nFastGRNNCell\nFSRNNCell\nLEMCell\ncoRNNCell\nAntisymmetricRNNCell\nGatedAntisymmetricRNNCell\nJANETCell\nCFNCell\nTRNNCell\nTGRUCell\nTLSTMCell\nUnICORNNCell","category":"page"},{"location":"api/cells/#RecurrentLayers.RANCell","page":"Cells","title":"RecurrentLayers.RANCell","text":"RANCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nRecurrent Additive Network cell. See RAN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntildec_t = W_c x_t \ni_t         = sigma(W_i x_t + U_i h_t-1 + b_i) \nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \nc_t         = i_t odot tildec_t + f_t odot c_t-1 \nh_t         = g(c_t)\nendaligned\n\nForward\n\nrancell(inp, (state, cstate))\nrancell(inp)\n\nArguments\n\ninp: The input to the rancell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.IndRNNCell","page":"Cells","title":"RecurrentLayers.IndRNNCell","text":"IndRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nIndependently recurrent cell. See IndRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nmathbfh_t = sigma(mathbfW mathbfx_t + mathbfu odot mathbfh_t-1 + mathbfb)\n\nForward\n\nindrnncell(inp, state)\nindrnncell(inp)\n\nArguments\n\ninp: The input to the indrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the IndRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.LightRUCell","page":"Cells","title":"RecurrentLayers.LightRUCell","text":"LightRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLight recurrent unit. See LightRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntildeh_t = tanh(W_h x_t) \nf_t         = delta(W_f x_t + U_f h_t-1 + b_f) \nh_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nForward\n\nlightrucell(inp, state)\nlightrucell(inp)\n\nArguments\n\ninp: The input to the lightrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the LightRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.LiGRUCell","page":"Cells","title":"RecurrentLayers.LiGRUCell","text":"LiGRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLight gated recurrent unit. The implementation does not include the batch normalization as described in the original paper. See LiGRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz_t = sigma(W_z x_t + U_z h_t-1) \ntildeh_t = textReLU(W_h x_t + U_h h_t-1) \nh_t = z_t odot h_t-1 + (1 - z_t) odot tildeh_t\nendaligned\n\nForward\n\nligrucell(inp, state)\nligrucell(inp)\n\nArguments\n\ninp: The input to the ligrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the LiGRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.MGUCell","page":"Cells","title":"RecurrentLayers.MGUCell","text":"MGUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMinimal gated unit. See MGU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \ntildeh_t = tanh(W_h x_t + U_h (f_t odot h_t-1) + b_h) \nh_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nForward\n\nmgucell(inp, state)\nmgucell(inp)\n\nArguments\n\ninp: The input to the mgucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MGUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.NASCell","page":"Cells","title":"RecurrentLayers.NASCell","text":"NASCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nNeural Architecture Search unit. See NAS for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntextFirst Layer Outputs  \no_1 = sigma(W_i^(1) x_t + W_h^(1) h_t-1 + b^(1)) \no_2 = textReLU(W_i^(2) x_t + W_h^(2) h_t-1 + b^(2)) \no_3 = sigma(W_i^(3) x_t + W_h^(3) h_t-1 + b^(3)) \no_4 = textReLU(W_i^(4) x_t cdot W_h^(4) h_t-1) \no_5 = tanh(W_i^(5) x_t + W_h^(5) h_t-1 + b^(5)) \no_6 = sigma(W_i^(6) x_t + W_h^(6) h_t-1 + b^(6)) \no_7 = tanh(W_i^(7) x_t + W_h^(7) h_t-1 + b^(7)) \no_8 = sigma(W_i^(8) x_t + W_h^(8) h_t-1 + b^(8)) \n\ntextSecond Layer Computations  \nl_1 = tanh(o_1 cdot o_2) \nl_2 = tanh(o_3 + o_4) \nl_3 = tanh(o_5 cdot o_6) \nl_4 = sigma(o_7 + o_8) \n\ntextInject Cell State  \nl_1 = tanh(l_1 + c_textstate) \n\ntextFinal Layer Computations  \nc_textnew = l_1 cdot l_2 \nl_5 = tanh(l_3 + l_4) \nh_textnew = tanh(c_textnew cdot l_5)\nendaligned\n\nForward\n\nnascell(inp, (state, cstate))\nnascell(inp)\n\nArguments\n\ninp: The input to the nascell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the NASCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.RHNCell","page":"Cells","title":"RecurrentLayers.RHNCell","text":"RHNCell(input_size => hidden_size, [depth];\n    couple_carry = true,\n    cell_kwargs...)\n\nRecurrent highway network. See RHNCellUnit for a the unit component of this layer. See RHN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndepth: depth of the recurrence. Default is 3\n\nKeyword arguments\n\ncouple_carry: couples the carry gate and the transform gate. Default true\ninit_kernel: initializer for the input to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ns_ell^t = h_ell^t odot t_ell^t + s_ell-1^t odot c_ell^t \ntextwhere \nh_ell^t = tanh(W_h x^tmathbbI_ell = 1 + U_h_ell s_ell-1^t + b_h_ell) \nt_ell^t = sigma(W_t x^tmathbbI_ell = 1 + U_t_ell s_ell-1^t + b_t_ell) \nc_ell^t = sigma(W_c x^tmathbbI_ell = 1 + U_c_ell s_ell-1^t + b_c_ell)\nendaligned\n\nForward\n\nrnncell(inp, [state])\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.RHNCellUnit","page":"Cells","title":"RecurrentLayers.RHNCellUnit","text":"RHNCellUnit(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    bias = true)\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.MUT1Cell","page":"Cells","title":"RecurrentLayers.MUT1Cell","text":"MUT1Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 1 cell. See MUT1 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + b_z) \nr = sigma(W_r x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + tanh(W_h x_t) + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.MUT2Cell","page":"Cells","title":"RecurrentLayers.MUT2Cell","text":"MUT2Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 2 cell. See MUT2 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + U_z h_t + b_z) \nr = sigma(x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.MUT3Cell","page":"Cells","title":"RecurrentLayers.MUT3Cell","text":"MUT3Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 3 cell. See MUT3 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + U_z tanh(h_t) + b_z) \nr = sigma(W_r x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.SCRNCell","page":"Cells","title":"RecurrentLayers.SCRNCell","text":"SCRNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, alpha = 0.0)\n\nStructurally contraint recurrent unit. See SCRN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nalpha: structural contraint. Default is 0.0\n\nEquations\n\nbeginaligned\ns_t = (1 - alpha) W_s x_t + alpha s_t-1 \nh_t = sigma(W_h s_t + U_h h_t-1 + b_h) \ny_t = f(U_y h_t + W_y s_t)\nendaligned\n\nForward\n\nscrncell(inp, (state, cstate))\nscrncell(inp)\n\nArguments\n\ninp: The input to the scrncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the SCRNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.PeepholeLSTMCell","page":"Cells","title":"RecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nPeephole long short term memory cell. See PeepholeLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nf_t = sigma_g(W_f x_t + U_f c_t-1 + b_f) \ni_t = sigma_g(W_i x_t + U_i c_t-1 + b_i) \no_t = sigma_g(W_o x_t + U_o c_t-1 + b_o) \nc_t = f_t odot c_t-1 + i_t odot sigma_c(W_c x_t + b_c) \nh_t = o_t odot sigma_h(c_t)\nendaligned\n\nForward\n\npeepholelstmcell(inp, (state, cstate))\npeepholelstmcell(inp)\n\nArguments\n\ninp: The input to the peepholelstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the PeepholeLSTMCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.FastRNNCell","page":"Cells","title":"RecurrentLayers.FastRNNCell","text":"FastRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nFast recurrent neural network cell. See FastRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: the activation function, defaults to tanh_fast\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntildeh_t = sigma(W_h x_t + U_h h_t-1 + b) \nh_t = alpha tildeh_t + beta h_t-1\nendaligned\n\nForward\n\nfastrnncell(inp, state)\nfastrnncell(inp)\n\nArguments\n\ninp: The input to the fastrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.FastGRNNCell","page":"Cells","title":"RecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nFast gated recurrent neural network cell. See FastGRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: the activation function, defaults to tanh_fast\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz_t = sigma(W x_t + U h_t-1 + b_z) \ntildeh_t = tanh(W x_t + U h_t-1 + b_h) \nh_t = big((zeta (1 - z_t) + nu) odot tildeh_tbig) + z_t odot h_t-1\nendaligned\n\nForward\n\nfastgrnncell(inp, state)\nfastgrnncell(inp)\n\nArguments\n\ninp: The input to the fastgrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastGRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.FSRNNCell","page":"Cells","title":"RecurrentLayers.FSRNNCell","text":"FSRNNCell(input_size => hidden_size,\n    fast_cells, slow_cell)\n\nFast slow recurrent neural network cell. See FSRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nfast_cells: a vector of the fast cells. Must be minimum of length 2.\nslow_cell: the chosen slow cell.\n\nEquations\n\nbeginaligned\n    h_t^F_1 = f^F_1left(h_t-1^F_k x_tright) \n    h_t^S = f^Sleft(h_t-1^S h_t^F_1right) \n    h_t^F_2 = f^F_2left(h_t^F_1 h_t^Sright) \n    h_t^F_i = f^F_ileft(h_t^F_i-1right) quad textfor  3 leq i leq k\nendaligned\n\nForward\n\nfsrnncell(inp, (fast_state, slow_state))\nfsrnncell(inp)\n\nArguments\n\ninp: The input to the fsrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(fast_state, slow_state): A tuple containing the hidden and cell states of the FSRNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (fast_state, slow_state) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.LEMCell","page":"Cells","title":"RecurrentLayers.LEMCell","text":"LEMCell(input_size => hidden_size, [dt];\n    init_kernel = glorot_uniform, init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLong expressive memory unit. See LEM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: timestep. Defaul is 1.0\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nboldsymbolDelta t_n = Delta hatt hatsigma\n    (W_1 y_n-1 + V_1 u_n + b_1) \noverlineboldsymbolDelta t_n = Delta hatt\n    hatsigma (W_2 y_n-1 + V_2 u_n + b_2) \nz_n = (1 - boldsymbolDelta t_n) odot z_n-1 +\n    boldsymbolDelta t_n odot sigma (W_z y_n-1 + V_z u_n + b_z) \ny_n = (1 - boldsymbolDelta t_n) odot y_n-1 +\n    boldsymbolDelta t_n odot sigma (W_y z_n + V_y u_n + b_y)\nendaligned\n\nForward\n\nlemcell(inp, (state, cstate))\nlemcell(inp)\n\nArguments\n\ninp: The input to the lemcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.coRNNCell","page":"Cells","title":"RecurrentLayers.coRNNCell","text":"coRNNCell(input_size => hidden_size, [dt];\n    gamma=0.0, epsilon=0.0,\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nCoupled oscillatory recurrent neural unit. See coRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\ngamma: damping for state. Default is 0.0.\nepsilon: damping for candidate state. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nmathbfy_n = y_n-1 + Delta t mathbfz_n \nmathbfz_n = z_n-1 + Delta t sigma left( mathbfW y_n-1 +\n    mathcalW z_n-1 + mathbfV u_n + mathbfb right) -\n    Delta t gamma y_n-1 - Delta t epsilon mathbfz_n\nendaligned\n\nForward\n\ncornncell(inp, (state, cstate))\ncornncell(inp)\n\nArguments\n\ninp: The input to the cornncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the coRNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.AntisymmetricRNNCell","page":"Cells","title":"RecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, epsilon=1.0)\n\nAntisymmetric recurrent cell. See AntisymmetricRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0\n\nEquations\n\nh_t = h_t-1 + epsilon tanh left( (W_h - W_h^T - gamma I) h_t-1 + V_h x_t + b_h right)\n\nForward\n\nasymrnncell(inp, state)\nasymrnncell(inp)\n\nArguments\n\ninp: The input to the asymrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the AntisymmetricRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.GatedAntisymmetricRNNCell","page":"Cells","title":"RecurrentLayers.GatedAntisymmetricRNNCell","text":"GatedAntisymmetricRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, epsilon=1.0)\n\nAntisymmetric recurrent cell with gating. See GatedAntisymmetricRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0\n\nEquations\n\nbeginaligned\n    z_t = sigma left( (W_h - W_h^T - gamma I) h_t-1 + V_z x_t + b_z right) \n    h_t = h_t-1 + epsilon z_t odot tanh left( (W_h - W_h^T - gamma I) h_t-1 + V_h x_t + b_h right)\nendaligned\n\nForward\n\nasymrnncell(inp, state)\nasymrnncell(inp)\n\nArguments\n\ninp: The input to the asymrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the GatedAntisymmetricRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.JANETCell","page":"Cells","title":"RecurrentLayers.JANETCell","text":"JANETCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, beta_value=1.0)\n\nJust another network unit. See JANET for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nbeta_value: control over the input data flow. Default is 1.0\n\nEquations\n\nbeginaligned\n    mathbfs_t = mathbfU_f mathbfh_t-1 + mathbfW_f mathbfx_t + mathbfb_f \n    tildemathbfc_t = tanh (mathbfU_c mathbfh_t-1 + mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfc_t = sigma(mathbfs_t) odot mathbfc_t-1 + (1 - sigma (mathbfs_t - beta)) odot tildemathbfc_t \n    mathbfh_t = mathbfc_t\nendaligned\n\nForward\n\njanetcell(inp, (state, cstate))\njanetcell(inp)\n\nArguments\n\ninp: The input to the rancell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.CFNCell","page":"Cells","title":"RecurrentLayers.CFNCell","text":"CFNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nChaos free network unit. See CFN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    h_t = theta_t odot tanh(h_t-1) + eta_t odot tanh(W x_t) \n    theta_t = sigma (U_theta h_t-1 + V_theta x_t + b_theta) \n    eta_t = sigma (U_eta h_t-1 + V_eta x_t + b_eta)\nendaligned\n\nForward\n\ncfncell(inp, state)\ncfncell(inp)\n\nArguments\n\ninp: The input to the cfncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the CFNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.TRNNCell","page":"Cells","title":"RecurrentLayers.TRNNCell","text":"TRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed recurrent unit. See TRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfW x_t \n    f_t = sigma (mathbfV x_t + b) \n    h_t = f_t odot h_t-1 + (1 - f_t) odot z_t\nendaligned\n\nForward\n\ntrnncell(inp, state)\ntrnncell(inp)\n\nArguments\n\ninp: The input to the trnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.TGRUCell","page":"Cells","title":"RecurrentLayers.TGRUCell","text":"TGRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed gated recurrent unit. See TGRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t + mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    h_t = f_t odot h_t-1 + z_t odot o_t\nendaligned\n\nForward\n\ntgrucell(inp, state)\ntgrucell(inp)\n\nArguments\n\ninp: The input to the tgrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TGRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, inp) is the new hidden state together with the current input.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.TLSTMCell","page":"Cells","title":"RecurrentLayers.TLSTMCell","text":"TLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed long short term memory cell. See TLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t + mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    c_t = f_t odot c_t-1 + (1 - f_t) odot z_t \n    h_t = c_t odot o_t\nendaligned\n\nForward\n\ntlstmcell(inp, state)\ntlstmcell(inp)\n\nArguments\n\ninp: The input to the tlstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TLSTMCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate, inp) is the new hidden and cell state, together with the current input.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#RecurrentLayers.UnICORNNCell","page":"Cells","title":"RecurrentLayers.UnICORNNCell","text":"UnICORNNCell(input_size => hidden_size, [dt];\n    alpha=0.0, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nUndamped independent controlled oscillatory recurrent neural unit. See coRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\nalpha: Control parameter. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    y_n = y_n-1 + Delta t  hatsigma(c) odot z_n \n    z_n = z_n-1 - Delta t  hatsigma(c) odot left \n        sigma left( w odot y_n-1 + V y_n-1 + b right) + \n        alpha y_n-1 right\nendaligned\n\nForward\n\nunicornncell(inp, (state, cstate))\nunicornncell(inp)\n\nArguments\n\ninp: The input to the unicornncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the UnICORNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"RAN\nIndRNN\nLightRU\nLiGRU\nMGU\nNAS\nRHN\nMUT1\nMUT2\nMUT3\nSCRN\nPeepholeLSTM\nFastRNN\nFastGRNN\nFSRNN\nLEM\ncoRNN\nAntisymmetricRNN\nGatedAntisymmetricRNN\nJANET\nCFN\nTRNN\nTGRU\nTLSTM\nUnICORNN","category":"page"},{"location":"api/layers/#RecurrentLayers.RAN","page":"Layers","title":"RecurrentLayers.RAN","text":"RAN(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nRecurrent Additive Network cell. See RANCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\ntildec_t = W_c x_t \ni_t         = sigma(W_i x_t + U_i h_t-1 + b_i) \nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \nc_t         = i_t odot tildec_t + f_t odot c_t-1 \nh_t         = g(c_t)\nendaligned\n\nForward\n\nran(inp, (state, cstate))\nran(inp)\n\nArguments\n\ninp: The input to the ran. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RAN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.IndRNN","page":"Layers","title":"RecurrentLayers.IndRNN","text":"IndRNN(input_size, hidden_size, [activation];\n    return_state = false, kwargs...)\n\nIndependently recurrent network. See IndRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nmathbfh_t = sigma(mathbfW mathbfx_t + mathbfu odot mathbfh_t-1 + mathbfb)\n\nForward\n\nindrnn(inp, state)\nindrnn(inp)\n\nArguments\n\ninp: The input to the indrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the IndRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.LightRU","page":"Layers","title":"RecurrentLayers.LightRU","text":"LightRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nLight recurrent unit network. See LightRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntildeh_t = tanh(W_h x_t) \nf_t         = delta(W_f x_t + U_f h_t-1 + b_f) \nh_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nForward\n\nlightru(inp, state)\nlightru(inp)\n\nArguments\n\ninp: The input to the lightru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the LightRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.LiGRU","page":"Layers","title":"RecurrentLayers.LiGRU","text":"LiGRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nLight gated recurrent network. The implementation does not include the batch normalization as described in the original paper. See LiGRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz_t = sigma(W_z x_t + U_z h_t-1) \ntildeh_t = textReLU(W_h x_t + U_h h_t-1) \nh_t = z_t odot h_t-1 + (1 - z_t) odot tildeh_t\nendaligned\n\nForward\n\nligru(inp, state)\nligru(inp)\n\nArguments\n\ninp: The input to the ligru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the LiGRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.MGU","page":"Layers","title":"RecurrentLayers.MGU","text":"MGU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nMinimal gated unit network. See MGUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \ntildeh_t = tanh(W_h x_t + U_h (f_t odot h_t-1) + b_h) \nh_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nForward\n\nmgu(inp, state)\nmgu(inp)\n\nArguments\n\ninp: The input to the mgu. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MGU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.NAS","page":"Layers","title":"RecurrentLayers.NAS","text":"NAS(input_size => hidden_size;\n    return_state = false,\n    kwargs...)\n\nNeural Architecture Search unit. See NASCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\ntextFirst Layer Outputs  \no_1 = sigma(W_i^(1) x_t + W_h^(1) h_t-1 + b^(1)) \no_2 = textReLU(W_i^(2) x_t + W_h^(2) h_t-1 + b^(2)) \no_3 = sigma(W_i^(3) x_t + W_h^(3) h_t-1 + b^(3)) \no_4 = textReLU(W_i^(4) x_t cdot W_h^(4) h_t-1) \no_5 = tanh(W_i^(5) x_t + W_h^(5) h_t-1 + b^(5)) \no_6 = sigma(W_i^(6) x_t + W_h^(6) h_t-1 + b^(6)) \no_7 = tanh(W_i^(7) x_t + W_h^(7) h_t-1 + b^(7)) \no_8 = sigma(W_i^(8) x_t + W_h^(8) h_t-1 + b^(8)) \n\ntextSecond Layer Computations  \nl_1 = tanh(o_1 cdot o_2) \nl_2 = tanh(o_3 + o_4) \nl_3 = tanh(o_5 cdot o_6) \nl_4 = sigma(o_7 + o_8) \n\ntextInject Cell State  \nl_1 = tanh(l_1 + c_textstate) \n\ntextFinal Layer Computations  \nc_textnew = l_1 cdot l_2 \nl_5 = tanh(l_3 + l_4) \nh_textnew = tanh(c_textnew cdot l_5)\nendaligned\n\nForward\n\nnas(inp, (state, cstate))\nnas(inp)\n\nArguments\n\ninp: The input to the nas. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the NAS.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.RHN","page":"Layers","title":"RecurrentLayers.RHN","text":"RHN(input_size => hidden_size, [depth];\n    return_state = false,\n    kwargs...)\n\nRecurrent highway network. See RHNCellUnit for a the unit component of this layer. See RHNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndepth: depth of the recurrence. Default is 3\n\nKeyword arguments\n\ncouple_carry: couples the carry gate and the transform gate. Default true\ninit_kernel: initializer for the input to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\ns_ell^t = h_ell^t odot t_ell^t + s_ell-1^t odot c_ell^t \ntextwhere \nh_ell^t = tanh(W_h x^tmathbbI_ell = 1 + U_h_ell s_ell-1^t + b_h_ell) \nt_ell^t = sigma(W_t x^tmathbbI_ell = 1 + U_t_ell s_ell-1^t + b_t_ell) \nc_ell^t = sigma(W_c x^tmathbbI_ell = 1 + U_c_ell s_ell-1^t + b_c_ell)\nendaligned\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.MUT1","page":"Layers","title":"RecurrentLayers.MUT1","text":"MUT1(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nMutated unit 1 network. See MUT1Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + b_z) \nr = sigma(W_r x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + tanh(W_h x_t) + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.MUT2","page":"Layers","title":"RecurrentLayers.MUT2","text":"MUT2Cell(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nMutated unit 2 network. See MUT2Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + U_z h_t + b_z) \nr = sigma(x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.MUT3","page":"Layers","title":"RecurrentLayers.MUT3","text":"MUT3(input_size => hidden_size;\nreturn_state = false, kwargs...)\n\nMutated unit 3 network. See MUT3Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nz = sigma(W_z x_t + U_z tanh(h_t) + b_z) \nr = sigma(W_r x_t + U_r h_t + b_r) \nh_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \nquad + h_t odot (1 - z)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.SCRN","page":"Layers","title":"RecurrentLayers.SCRN","text":"SCRN(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, alpha = 0.0,\n    return_state = false)\n\nStructurally contraint recurrent unit. See SCRNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nalpha: structural contraint. Default is 0.0\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\ns_t = (1 - alpha) W_s x_t + alpha s_t-1 \nh_t = sigma(W_h s_t + U_h h_t-1 + b_h) \ny_t = f(U_y h_t + W_y s_t)\nendaligned\n\nForward\n\nscrn(inp, (state, cstate))\nscrn(inp)\n\nArguments\n\ninp: The input to the scrn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the SCRN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.PeepholeLSTM","page":"Layers","title":"RecurrentLayers.PeepholeLSTM","text":"PeepholeLSTM(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nPeephole long short term memory network. See PeepholeLSTMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nf_t = sigma_g(W_f x_t + U_f c_t-1 + b_f) \ni_t = sigma_g(W_i x_t + U_i c_t-1 + b_i) \no_t = sigma_g(W_o x_t + U_o c_t-1 + b_o) \nc_t = f_t odot c_t-1 + i_t odot sigma_c(W_c x_t + b_c) \nh_t = o_t odot sigma_h(c_t)\nendaligned\n\nForward\n\npeepholelstm(inp, (state, cstate))\npeepholelstm(inp)\n\nArguments\n\ninp: The input to the peepholelstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the PeepholeLSTM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.FastRNN","page":"Layers","title":"RecurrentLayers.FastRNN","text":"FastRNN(input_size => hidden_size, [activation];\n    return_state = false, kwargs...)\n\nFast recurrent neural network. See FastRNNCell for a layer that processes a single sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: the activation function, defaults to tanh_fast\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\ntildeh_t = sigma(W_h x_t + U_h h_t-1 + b) \nh_t = alpha tildeh_t + beta h_t-1\nendaligned\n\nForward\n\nfastrnn(inp, state)\nfastrnn(inp)\n\nArguments\n\ninp: The input to the fastrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the FastRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.FastGRNN","page":"Layers","title":"RecurrentLayers.FastGRNN","text":"FastGRNN(input_size => hidden_size, [activation];\nreturn_state = false, kwargs...)\n\nFast recurrent neural network. See FastGRNNCell for a layer that processes a single sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: the activation function, defaults to tanh_fast\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\nz_t = sigma(W_z x_t + U_z h_t-1 + b_z) \ntildeh_t = tanh(W_h x_t + U_h h_t-1 + b_h) \nh_t = big((zeta (1 - z_t) + nu) odot tildeh_tbig) + z_t odot h_t-1\nendaligned\n\nForward\n\nfastgrnn(inp, state)\nfastgrnn(inp)\n\nArguments\n\ninp: The input to the fastgrnn. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastGRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.FSRNN","page":"Layers","title":"RecurrentLayers.FSRNN","text":"FSRNN(input_size => hidden_size,\n    fast_cells, slow_cell;\n    return_state=false)\n\nFast slow recurrent neural network. See FSRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nfast_cells: a vector of the fast cells. Must be minimum of length 2.\nslow_cell: the chosen slow cell.\nreturn_state: option to return the last state. Default is false.\n\nEquations\n\nbeginaligned\n    h_t^F_1 = f^F_1left(h_t-1^F_k x_tright) \n    h_t^S = f^Sleft(h_t-1^S h_t^F_1right) \n    h_t^F_2 = f^F_2left(h_t^F_1 h_t^Sright) \n    h_t^F_i = f^F_ileft(h_t^F_i-1right) quad textfor  3 leq i leq k\nendaligned\n\nForward\n\nfsrnn(inp, (fast_state, slow_state))\nfsrnn(inp)\n\nArguments\n\ninp: The input to the fsrnn. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(fast_state, slow_state): A tuple containing the hidden and cell states of the FSRNN. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.LEM","page":"Layers","title":"RecurrentLayers.LEM","text":"LEM(input_size => hidden_size, [dt];\n    return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nLong expressive memory network. See LEMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: timestep. Defaul is 1.0\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nboldsymbolDelta t_n = Delta hatt hatsigma\n    (W_1 y_n-1 + V_1 u_n + b_1) \noverlineboldsymbolDelta t_n = Delta hatt\n    hatsigma (W_2 y_n-1 + V_2 u_n + b_2) \nz_n = (1 - boldsymbolDelta t_n) odot z_n-1 +\n    boldsymbolDelta t_n odot sigma (W_z y_n-1 + V_z u_n + b_z) \ny_n = (1 - boldsymbolDelta t_n) odot y_n-1 +\n    boldsymbolDelta t_n odot sigma (W_y z_n + V_y u_n + b_y)\nendaligned\n\nForward\n\nlem(inp, (state, zstate))\nlem(inp)\n\nArguments\n\ninp: The input to the LEM. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the LEM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.coRNN","page":"Layers","title":"RecurrentLayers.coRNN","text":"coRNN(input_size => hidden_size, [dt];\n    gamma=0.0, epsilon=0.0,\n    return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nCoupled oscillatory recurrent neural unit. See coRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\ngamma: damping for state. Default is 0.0.\nepsilon: damping for candidate state. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\nmathbfy_n = y_n-1 + Delta t mathbfz_n \nmathbfz_n = z_n-1 + Delta t sigma left( mathbfW y_n-1 +\n    mathcalW z_n-1 + mathbfV u_n + mathbfb right) -\n    Delta t gamma y_n-1 - Delta t epsilon mathbfz_n\nendaligned\n\nForward\n\ncornn(inp, (state, zstate))\ncornn(inp)\n\nArguments\n\ninp: The input to the cornn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the coRNN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.AntisymmetricRNN","page":"Layers","title":"RecurrentLayers.AntisymmetricRNN","text":"AntisymmetricRNN(input_size, hidden_size, [activation];\n    return_state = false, kwargs...)\n\nAntisymmetric recurrent neural network. See AntisymmetricRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0\n\nEquations\n\nh_t = h_t-1 + epsilon tanh left( (W_h - W_h^T - gamma I) h_t-1 + V_h x_t + b_h right)\n\nForward\n\nasymrnn(inp, state)\nasymrnn(inp)\n\nArguments\n\ninp: The input to the asymrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the AntisymmetricRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.GatedAntisymmetricRNN","page":"Layers","title":"RecurrentLayers.GatedAntisymmetricRNN","text":"GatedAntisymmetricRNN(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nAntisymmetric recurrent neural network with gating. See GatedAntisymmetricRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0\n\nEquations\n\nbeginaligned\n    z_t = sigma left( (W_h - W_h^T - gamma I) h_t-1 + V_z x_t + b_z right) \n    h_t = h_t-1 + epsilon z_t odot tanh left( (W_h - W_h^T - gamma I) h_t-1 + V_h x_t + b_h right)\nendaligned\n\nForward\n\nasymrnn(inp, state)\nasymrnn(inp)\n\nArguments\n\ninp: The input to the asymrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the GatedAntisymmetricRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.JANET","page":"Layers","title":"RecurrentLayers.JANET","text":"JANET(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nJust another network. See JANETCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\nbeta_value: control over the input data flow. Default is 1.0\n\nEquations\n\nbeginaligned\n    mathbfs_t = mathbfU_f mathbfh_t-1 + mathbfW_f mathbfx_t + mathbfb_f \n    tildemathbfc_t = tanh (mathbfU_c mathbfh_t-1 + mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfc_t = sigma(mathbfs_t) odot mathbfc_t-1 + (1 - sigma (mathbfs_t - beta)) odot tildemathbfc_t \n    mathbfh_t = mathbfc_t\nendaligned\n\nForward\n\njanet(inp, (state, cstate))\njanet(inp)\n\nArguments\n\ninp: The input to the janet. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the JANET.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.CFN","page":"Layers","title":"RecurrentLayers.CFN","text":"CFN(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nChaos free network unit. See CFNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    h_t = theta_t odot tanh(h_t-1) + eta_t odot tanh(W x_t) \n    theta_t = sigma (U_theta h_t-1 + V_theta x_t + b_theta) \n    eta_t = sigma (U_eta h_t-1 + V_eta x_t + b_eta)\nendaligned\n\nForward\n\ncfn(inp, state)\ncfn(inp)\n\nArguments\n\ninp: The input to the cfn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the CFN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.TRNN","page":"Layers","title":"RecurrentLayers.TRNN","text":"TRNN(input_size => hidden_size, [activation];\n    return_state = false, kwargs...)\n\nStrongly typed recurrent unit. See TRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfW x_t \n    f_t = sigma (mathbfV x_t + b) \n    h_t = f_t odot h_t-1 + (1 - f_t) odot z_t\nendaligned\n\nForward\n\ntrnn(inp, state)\ntrnn(inp)\n\nArguments\n\ninp: The input to the trnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.TGRU","page":"Layers","title":"RecurrentLayers.TGRU","text":"TGRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nStrongly typed recurrent gated unit. See TGRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t + mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    h_t = f_t odot h_t-1 + z_t odot o_t\nendaligned\n\nForward\n\ntgru(inp, state)\ntgru(inp)\n\nArguments\n\ninp: The input to the tgru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TGRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.TLSTM","page":"Layers","title":"RecurrentLayers.TLSTM","text":"TLSTM(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nStrongly typed long short term memory. See TLSTMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t + mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    c_t = f_t odot c_t-1 + (1 - f_t) odot z_t \n    h_t = c_t odot o_t\nendaligned\n\nForward\n\ntlstm(inp, state)\ntlstm(inp)\n\nArguments\n\ninp: The input to the tlstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TLSTM. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden states new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#RecurrentLayers.UnICORNN","page":"Layers","title":"RecurrentLayers.UnICORNN","text":"UnICORNN(input_size => hidden_size, [dt];\n    alpha=0.0, return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nUndamped independent controlled oscillatory recurrent neural network. See UnICORNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\nalpha: Control parameter. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    y_n = y_n-1 + Delta t  hatsigma(c) odot z_n \n    z_n = z_n-1 - Delta t  hatsigma(c) odot left \n        sigma left( w odot y_n-1 + V y_n-1 + b right) + \n        alpha y_n-1 right\nendaligned\n\nForward\n\nunicornn(inp, (state, zstate))\nunicornn(inp)\n\nArguments\n\ninp: The input to the unicornn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the UnICORNN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"roadmap/#Roadmap","page":"Roadmap","title":"Roadmap","text":"","category":"section"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"This page documents some planned work for RecurrentLayers.jl. Future work for this library includes additional cells such as:","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"FastRNNs and FastGRUs (current focus) arxiv\nUnitary recurrent neural networks arxiv\nModern recurrent neural networks such as LRU  and minLSTM/minGRU\nQuasi recurrent neural networks arxiv","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"Additionally, some cell-independent architectures are also planned, that expand the ability of recurrent architectures and could theoretically take any cell:","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"Clockwork rnns arxiv\nPhased rnns arxiv\nSegment rnn arxiv\nFast-Slow rnns arxiv","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"An implementation of these ideally would be, for example FastSlow(RNNCell, input_size => hidden_size). More details on this soon!","category":"page"},{"location":"api/wrappers/#Wrappers","page":"Wrappers","title":"Wrappers","text":"","category":"section"},{"location":"api/wrappers/","page":"Wrappers","title":"Wrappers","text":"StackedRNN","category":"page"},{"location":"api/wrappers/#RecurrentLayers.StackedRNN","page":"Wrappers","title":"RecurrentLayers.StackedRNN","text":"StackedRNN(rlayer, (input_size, hidden_size), args...;\n    num_layers = 1, dropout = 0.0, kwargs...)\n\nConstructs a stack of recurrent layers given the recurrent layer type.\n\nArguments:\n\nrlayer: Any recurrent layer such as MGU, RHN, etc... or Flux.RNN, Flux.LSTM, etc.\ninput_size: Defines the input dimension for the first layer.\nhidden_size: defines the dimension of the hidden layer.\nnum_layers: The number of layers to stack. Default is 1.\ndropout: Value of dropout to apply between recurrent layers. Default is 0.0.\nargs...: Additional positional arguments passed to the recurrent layer.\n\nKeyword arguments\n\nkwargs...: Additional keyword arguments passed to the recurrent layers.\n\nExamples\n\njulia> using RecurrentLayers\n\njulia> stac_rnn = StackedRNN(MGU, (3=>5); num_layers = 4)\nStackedRNN(\n  [\n    MGU(3 => 10),                       # 90 parameters\n    MGU(5 => 10),                       # 110 parameters\n    MGU(5 => 10),                       # 110 parameters\n    MGU(5 => 10),                       # 110 parameters\n  ],\n)         # Total: 12 trainable arrays, 420 parameters,\n          # plus 4 non-trainable, 20 parameters, summarysize 2.711 KiB.\n\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RecurrentLayers","category":"page"},{"location":"#RecurrentLayers","page":"Home","title":"RecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RecurrentLayers.jl extends Flux.jl recurrent layers offering by providing implementations of additional recurrent layers not available in base deep learning libraries.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package offers multiple layers for Flux.jl. Currently there are 20+ cells implemented, together with multiple higher level implementations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modifications of vanilla RNNs: Independently recurrent neural networks, Structurally constrained recurrent neural network, FastRNN, and Typed RNNs.\nVariations over gated architectures: Minimal gated unit, Light gated recurrent networks, Recurrent addictive networks, Light recurrent networks, Neural architecture search networks, Evolving recurrent neural networks, Peephole long short term memory, FastGRNN, Just another network, Chaos free network, Typed gated recurrent unit, and Typed long short term memory.\nDiscretized ordinary differential equation formulations of RNNs: Long expressive memory networks,  Coupled oscillatory recurrent neural unit, Antisymmetric recurrent neural network with its gated version, and Undamped independent controlled oscillatory recurrent neural network.\nAdditional more complex architectures: Recurrent highway networks, and FastSlow RNNs\nAdditional wrappers: Stacked RNNs","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install RecurrentLayers using either of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RecurrentLayers\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add RecurrentLayers","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are always welcome! We specifically look for :","category":"page"},{"location":"","page":"Home","title":"Home","text":"Recurrent cells you would like to see implemented \nBenchmarks\nFixes for any bugs/errors\nDocumentation, in any form: examples, how tos, docstrings","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please consider the following guidelines before opening a pull request:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The code should be formatted according to the format file provided\nVariable names should be meaningful: please no single letter variables, and try to avoid double letters variables too. I know at the moment there are some in the codebase, but I will need a breaking change in order to fix the majority of them.\nThe format file does not format markdown. If you are adding docs, or docstrings please take care of not going over 92 cols.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For any clarification feel free to contact me directly (@MartinuzziFrancesco) either in the julia slack, by email or X/bluesky.","category":"page"}]
}
