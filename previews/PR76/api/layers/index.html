<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Layers · RecurrentLayers.jl</title><meta name="title" content="Layers · RecurrentLayers.jl"/><meta property="og:title" content="Layers · RecurrentLayers.jl"/><meta property="twitter:title" content="Layers · RecurrentLayers.jl"/><meta name="description" content="Documentation for RecurrentLayers.jl."/><meta property="og:description" content="Documentation for RecurrentLayers.jl."/><meta property="twitter:description" content="Documentation for RecurrentLayers.jl."/><meta property="og:url" content="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/layers/"/><meta property="twitter:url" content="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/layers/"/><link rel="canonical" href="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/layers/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="RecurrentLayers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RecurrentLayers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Documentation</span><ul><li><a class="tocitem" href="../cells/">Cells</a></li><li class="is-active"><a class="tocitem" href>Layers</a></li><li><a class="tocitem" href="../wrappers/">Wrappers</a></li></ul></li><li><a class="tocitem" href="../../roadmap/">Roadmap</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Layers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/main/docs/src/api/layers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.RAN" href="#RecurrentLayers.RAN"><code>RecurrentLayers.RAN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RAN(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1705.07393">Recurrent Additive Network cell</a>. See <a href="../cells/#RecurrentLayers.RANCell"><code>RANCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{c}_t &amp;= W_c x_t, \\
i_t         &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i), \\
f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
c_t         &amp;= i_t \odot \tilde{c}_t + f_t \odot c_{t-1}, \\
h_t         &amp;= g(c_t)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">ran(inp, (state, cstate))
ran(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the ran. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the RAN.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/ran_cell.jl#L85-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.IndRNN" href="#RecurrentLayers.IndRNN"><code>RecurrentLayers.IndRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndRNN(input_size, hidden_size, [activation];
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1803.04831">Independently recurrent network</a>. See <a href="../cells/#RecurrentLayers.IndRNNCell"><code>IndRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\mathbf{h}_{t} = \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{u} \odot \mathbf{h}_{t-1} + \mathbf{b})\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">indrnn(inp, state)
indrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the indrnn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the IndRNN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/indrnn_cell.jl#L77-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LightRU" href="#RecurrentLayers.LightRU"><code>RecurrentLayers.LightRU</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LightRU(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://www.mdpi.com/2079-9292/13/16/3204">Light recurrent unit network</a>. See <a href="../cells/#RecurrentLayers.LightRUCell"><code>LightRUCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{h}_t &amp;= \tanh(W_h x_t), \\
f_t         &amp;= \delta(W_f x_t + U_f h_{t-1} + b_f), \\
h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">lightru(inp, state)
lightru(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the lightru. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the LightRU. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/lightru_cell.jl#L84-L129">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LiGRU" href="#RecurrentLayers.LiGRU"><code>RecurrentLayers.LiGRU</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LiGRU(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1803.10225">Light gated recurrent network</a>. The implementation does not include the batch normalization as described in the original paper. See <a href="../cells/#RecurrentLayers.LiGRUCell"><code>LiGRUCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z_t &amp;= \sigma(W_z x_t + U_z h_{t-1}), \\
\tilde{h}_t &amp;= \text{ReLU}(W_h x_t + U_h h_{t-1}), \\
h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">ligru(inp, state)
ligru(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the ligru. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the LiGRU. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/ligru_cell.jl#L84-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MGU" href="#RecurrentLayers.MGU"><code>RecurrentLayers.MGU</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MGU(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1603.09420">Minimal gated unit network</a>. See <a href="../cells/#RecurrentLayers.MGUCell"><code>MGUCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
\tilde{h}_t &amp;= \tanh(W_h x_t + U_h (f_t \odot h_{t-1}) + b_h), \\
h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mgu(inp, state)
mgu(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mgu. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the MGU. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/mgu_cell.jl#L82-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.NAS" href="#RecurrentLayers.NAS"><code>RecurrentLayers.NAS</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NAS(input_size =&gt; hidden_size;
    return_state = false,
    kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1611.01578">Neural Architecture Search unit</a>. See <a href="../cells/#RecurrentLayers.NASCell"><code>NASCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\text{First Layer Outputs:} &amp; \\
o_1 &amp;= \sigma(W_i^{(1)} x_t + W_h^{(1)} h_{t-1} + b^{(1)}), \\
o_2 &amp;= \text{ReLU}(W_i^{(2)} x_t + W_h^{(2)} h_{t-1} + b^{(2)}), \\
o_3 &amp;= \sigma(W_i^{(3)} x_t + W_h^{(3)} h_{t-1} + b^{(3)}), \\
o_4 &amp;= \text{ReLU}(W_i^{(4)} x_t \cdot W_h^{(4)} h_{t-1}), \\
o_5 &amp;= \tanh(W_i^{(5)} x_t + W_h^{(5)} h_{t-1} + b^{(5)}), \\
o_6 &amp;= \sigma(W_i^{(6)} x_t + W_h^{(6)} h_{t-1} + b^{(6)}), \\
o_7 &amp;= \tanh(W_i^{(7)} x_t + W_h^{(7)} h_{t-1} + b^{(7)}), \\
o_8 &amp;= \sigma(W_i^{(8)} x_t + W_h^{(8)} h_{t-1} + b^{(8)}). \\

\text{Second Layer Computations:} &amp; \\
l_1 &amp;= \tanh(o_1 \cdot o_2) \\
l_2 &amp;= \tanh(o_3 + o_4) \\
l_3 &amp;= \tanh(o_5 \cdot o_6) \\
l_4 &amp;= \sigma(o_7 + o_8) \\

\text{Inject Cell State:} &amp; \\
l_1 &amp;= \tanh(l_1 + c_{\text{state}}) \\

\text{Final Layer Computations:} &amp; \\
c_{\text{new}} &amp;= l_1 \cdot l_2 \\
l_5 &amp;= \tanh(l_3 + l_4) \\
h_{\text{new}} &amp;= \tanh(c_{\text{new}} \cdot l_5)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">nas(inp, (state, cstate))
nas(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the nas. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the NAS.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/nas_cell.jl#L150-L217">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.RHN" href="#RecurrentLayers.RHN"><code>RecurrentLayers.RHN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RHN(input_size =&gt; hidden_size, [depth];
    return_state = false,
    kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1607.03474">Recurrent highway network</a>. See <a href="../cells/#RecurrentLayers.RHNCellUnit"><code>RHNCellUnit</code></a> for a the unit component of this layer. See <a href="../cells/#RecurrentLayers.RHNCell"><code>RHNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>depth</code>: depth of the recurrence. Default is 3</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>couple_carry</code>: couples the carry gate and the transform gate. Default <code>true</code></li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_{\ell}^{[t]} &amp;= h_{\ell}^{[t]} \odot t_{\ell}^{[t]} + s_{\ell-1}^{[t]} \odot c_{\ell}^{[t]}, \\
\text{where} \\
h_{\ell}^{[t]} &amp;= \tanh(W_h x^{[t]}\mathbb{I}_{\ell = 1} + U_{h_{\ell}} s_{\ell-1}^{[t]} + b_{h_{\ell}}), \\
t_{\ell}^{[t]} &amp;= \sigma(W_t x^{[t]}\mathbb{I}_{\ell = 1} + U_{t_{\ell}} s_{\ell-1}^{[t]} + b_{t_{\ell}}), \\
c_{\ell}^{[t]} &amp;= \sigma(W_c x^{[t]}\mathbb{I}_{\ell = 1} + U_{c_{\ell}} s_{\ell-1}^{[t]} + b_{c_{\ell}})
\end{aligned}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/rhn_cell.jl#L144-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT1" href="#RecurrentLayers.MUT1"><code>RecurrentLayers.MUT1</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT1(input_size =&gt; hidden_size;
    return_state=false,
    kwargs...)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 1 network</a>. See <a href="../cells/#RecurrentLayers.MUT1Cell"><code>MUT1Cell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + b_z), \\
r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + \tanh(W_h x_t) + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mut(inp, state)
mut(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mut. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUT. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/mut_cell.jl#L86-L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT2" href="#RecurrentLayers.MUT2"><code>RecurrentLayers.MUT2</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT2Cell(input_size =&gt; hidden_size;
    return_state=false,
    kwargs...)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 network</a>. See <a href="../cells/#RecurrentLayers.MUT2Cell"><code>MUT2Cell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
r &amp;= \sigma(x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mut(inp, state)
mut(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mut. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUT. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/mut_cell.jl#L240-L287">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT3" href="#RecurrentLayers.MUT3"><code>RecurrentLayers.MUT3</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT3(input_size =&gt; hidden_size;
return_state = false, kwargs...)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 3 network</a>. See <a href="../cells/#RecurrentLayers.MUT3Cell"><code>MUT3Cell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + U_z \tanh(h_t) + b_z), \\
r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mut(inp, state)
mut(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mut. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUT. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/mut_cell.jl#L393-L439">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.SCRN" href="#RecurrentLayers.SCRN"><code>RecurrentLayers.SCRN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SCRN(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, alpha = 0.0,
    return_state = false)</code></pre><p><a href="https://arxiv.org/pdf/1412.7753">Structurally contraint recurrent unit</a>. See <a href="../cells/#RecurrentLayers.SCRNCell"><code>SCRNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>alpha</code>: structural contraint. Default is 0.0</li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_t &amp;= (1 - \alpha) W_s x_t + \alpha s_{t-1}, \\
h_t &amp;= \sigma(W_h s_t + U_h h_{t-1} + b_h), \\
y_t &amp;= f(U_y h_t + W_y s_t)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">scrn(inp, (state, cstate))
scrn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the scrn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the SCRN.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/scrn_cell.jl#L91-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.PeepholeLSTM" href="#RecurrentLayers.PeepholeLSTM"><code>RecurrentLayers.PeepholeLSTM</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PeepholeLSTM(input_size =&gt; hidden_size;
    return_state=false,
    kwargs...)</code></pre><p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Peephole long short term memory network</a>. See <a href="../cells/#RecurrentLayers.PeepholeLSTMCell"><code>PeepholeLSTMCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
f_t &amp;= \sigma_g(W_f x_t + U_f c_{t-1} + b_f), \\
i_t &amp;= \sigma_g(W_i x_t + U_i c_{t-1} + b_i), \\
o_t &amp;= \sigma_g(W_o x_t + U_o c_{t-1} + b_o), \\
c_t &amp;= f_t \odot c_{t-1} + i_t \odot \sigma_c(W_c x_t + b_c), \\
h_t &amp;= o_t \odot \sigma_h(c_t).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">peepholelstm(inp, (state, cstate))
peepholelstm(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the peepholelstm. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the PeepholeLSTM.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/peepholelstm_cell.jl#L83-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FastRNN" href="#RecurrentLayers.FastRNN"><code>RecurrentLayers.FastRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastRNN(input_size =&gt; hidden_size, [activation];
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast recurrent neural network</a>. See <a href="../cells/#RecurrentLayers.FastRNNCell"><code>FastRNNCell</code></a> for a layer that processes a single sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{h}_t &amp;= \sigma(W_h x_t + U_h h_{t-1} + b), \\
h_t &amp;= \alpha \tilde{h}_t + \beta h_{t-1}
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fastrnn(inp, state)
fastrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fastrnn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the FastRNN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/fastrnn_cell.jl#L89-L134">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FastGRNN" href="#RecurrentLayers.FastGRNN"><code>RecurrentLayers.FastGRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastGRNN(input_size =&gt; hidden_size, [activation];
return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast recurrent neural network</a>. See <a href="../cells/#RecurrentLayers.FastGRNNCell"><code>FastGRNNCell</code></a> for a layer that processes a single sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z_t &amp;= \sigma(W_z x_t + U_z h_{t-1} + b_z), \\
\tilde{h}_t &amp;= \tanh(W_h x_t + U_h h_{t-1} + b_h), \\
h_t &amp;= \big((\zeta (1 - z_t) + \nu) \odot \tilde{h}_t\big) + z_t \odot h_{t-1}
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fastgrnn(inp, state)
fastgrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fastgrnn. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the FastGRNN. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/fastrnn_cell.jl#L252-L299">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FSRNN" href="#RecurrentLayers.FSRNN"><code>RecurrentLayers.FSRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FSRNN(input_size =&gt; hidden_size,
    fast_cells, slow_cell;
    return_state=false)</code></pre><p><a href="https://arxiv.org/abs/1705.08639">Fast slow recurrent neural network</a>. See <a href="../cells/#RecurrentLayers.FSRNNCell"><code>FSRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>fast_cells</code>: a vector of the fast cells. Must be minimum of length 2.</li><li><code>slow_cell</code>: the chosen slow cell.</li><li><code>return_state</code>: option to return the last state. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    h_t^{F_1} &amp;= f^{F_1}\left(h_{t-1}^{F_k}, x_t\right) \\
    h_t^S &amp;= f^S\left(h_{t-1}^S, h_t^{F_1}\right) \\
    h_t^{F_2} &amp;= f^{F_2}\left(h_t^{F_1}, h_t^S\right) \\
    h_t^{F_i} &amp;= f^{F_i}\left(h_t^{F_{i-1}}\right) \quad \text{for } 3 \leq i \leq k
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fsrnn(inp, (fast_state, slow_state))
fsrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fsrnn. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(fast_state, slow_state)</code>: A tuple containing the hidden and cell states of the FSRNN. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/fsrnn_cell.jl#L83-L125">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LEM" href="#RecurrentLayers.LEM"><code>RecurrentLayers.LEM</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LEM(input_size =&gt; hidden_size, [dt];
    return_state=false, init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform, bias = true)</code></pre><p><a href="https://arxiv.org/pdf/2110.04744">Long expressive memory network</a>. See <a href="../cells/#RecurrentLayers.LEMCell"><code>LEMCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>dt</code>: timestep. Defaul is 1.0</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\boldsymbol{\Delta t_n} &amp;= \Delta \hat{t} \hat{\sigma}
    (W_1 y_{n-1} + V_1 u_n + b_1) \\
\overline{\boldsymbol{\Delta t_n}} &amp;= \Delta \hat{t}
    \hat{\sigma} (W_2 y_{n-1} + V_2 u_n + b_2) \\
z_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot z_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_z y_{n-1} + V_z u_n + b_z) \\
y_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot y_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_y z_n + V_y u_n + b_y)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">lem(inp, (state, zstate))
lem(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the LEM. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the LEM.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/lem_cell.jl#L94-L147">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.coRNN" href="#RecurrentLayers.coRNN"><code>RecurrentLayers.coRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coRNN(input_size =&gt; hidden_size, [dt];
    gamma=0.0, epsilon=0.0,
    return_state=false, init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform, bias = true)</code></pre><p><a href="https://arxiv.org/abs/2010.00951">Coupled oscillatory recurrent neural unit</a>. See <a href="../cells/#RecurrentLayers.coRNNCell"><code>coRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>dt</code>: time step. Default is 1.0.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>gamma</code>: damping for state. Default is 0.0.</li><li><code>epsilon</code>: damping for candidate state. Default is 0.0.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\mathbf{y}_n &amp;= y_{n-1} + \Delta t \mathbf{z}_n, \\
\mathbf{z}_n &amp;= z_{n-1} + \Delta t \sigma \left( \mathbf{W} y_{n-1} +
    \mathcal{W} z_{n-1} + \mathbf{V} u_n + \mathbf{b} \right) -
    \Delta t \gamma y_{n-1} - \Delta t \epsilon \mathbf{z}_n,
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">cornn(inp, (state, zstate))
cornn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the <code>cornn</code>. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the <code>coRNN</code>.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/cornn_cell.jl#L93-L144">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.AntisymmetricRNN" href="#RecurrentLayers.AntisymmetricRNN"><code>RecurrentLayers.AntisymmetricRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AntisymmetricRNN(input_size, hidden_size, [activation];
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent neural network</a>. See <a href="../cells/#RecurrentLayers.AntisymmetricRNNCell"><code>AntisymmetricRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0</li></ul><p><strong>Equations</strong></p><p class="math-container">\[h_t = h_{t-1} + \epsilon \tanh \left( (W_h - W_h^T - \gamma I) h_{t-1} + V_h x_t + b_h \right),\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">asymrnn(inp, state)
asymrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the asymrnn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the AntisymmetricRNN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/antisymmetricrnn_cell.jl#L86-L128">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.GatedAntisymmetricRNN" href="#RecurrentLayers.GatedAntisymmetricRNN"><code>RecurrentLayers.GatedAntisymmetricRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GatedAntisymmetricRNN(input_size, hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent neural network with gating</a>. See <a href="../cells/#RecurrentLayers.GatedAntisymmetricRNNCell"><code>GatedAntisymmetricRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \sigma \left( (W_h - W_h^T - \gamma I) h_{t-1} + V_z x_t + b_z \right), \\
    h_t &amp;= h_{t-1} + \epsilon z_t \odot \tanh \left( (W_h - W_h^T - \gamma I) h_{t-1} + V_h x_t + b_h \right).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">asymrnn(inp, state)
asymrnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the asymrnn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the GatedAntisymmetricRNN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/antisymmetricrnn_cell.jl#L242-L286">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.JANET" href="#RecurrentLayers.JANET"><code>RecurrentLayers.JANET</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">JANET(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1804.04849">Just another network</a>. See <a href="../cells/#RecurrentLayers.JANETCell"><code>JANETCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>beta_value</code>: control over the input data flow. Default is 1.0</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{s}_t &amp;= \mathbf{U}_f \mathbf{h}_{t-1} + \mathbf{W}_f \mathbf{x}_t + \mathbf{b}_f \\
    \tilde{\mathbf{c}}_t &amp;= \tanh (\mathbf{U}_c \mathbf{h}_{t-1} + \mathbf{W}_c \mathbf{x}_t + \mathbf{b}_c) \\
    \mathbf{c}_t &amp;= \sigma(\mathbf{s}_t) \odot \mathbf{c}_{t-1} + (1 - \sigma (\mathbf{s}_t - \beta)) \odot \tilde{\mathbf{c}}_t \\
    \mathbf{h}_t &amp;= \mathbf{c}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">janet(inp, (state, cstate))
janet(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the janet. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the JANET.  They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/janet_cell.jl#L93-L141">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.CFN" href="#RecurrentLayers.CFN"><code>RecurrentLayers.CFN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CFN(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1612.06212">Chaos free network unit</a>. See <a href="../cells/#RecurrentLayers.CFNCell"><code>CFNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    h_t &amp;= \theta_t \odot \tanh(h_{t-1}) + \eta_t \odot \tanh(W x_t), \\
    \theta_t &amp;:= \sigma (U_\theta h_{t-1} + V_\theta x_t + b_\theta), \\
    \eta_t &amp;:= \sigma (U_\eta h_{t-1} + V_\eta x_t + b_\eta).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">cfn(inp, state)
cfn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the cfn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the CFN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/cfn_cell.jl#L83-L129">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TRNN" href="#RecurrentLayers.TRNN"><code>RecurrentLayers.TRNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TRNN(input_size =&gt; hidden_size, [activation];
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed recurrent unit</a>. See <a href="../cells/#RecurrentLayers.TRNNCell"><code>TRNNCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: activation function. Default is <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{W} x_t \\
    f_t &amp;= \sigma (\mathbf{V} x_t + b) \\
    h_t &amp;= f_t \odot h_{t-1} + (1 - f_t) \odot z_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">trnn(inp, state)
trnn(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the trnn. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the TRNN. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/trnn_cell.jl#L81-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TGRU" href="#RecurrentLayers.TGRU"><code>RecurrentLayers.TGRU</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TGRU(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed recurrent gated unit</a>. See <a href="../cells/#RecurrentLayers.TGRUCell"><code>TGRUCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t + \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    h_t &amp;= f_t \odot h_{t-1} + z_t \odot o_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">tgru(inp, state)
tgru(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the tgru. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the TGRU. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden stats <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/trnn_cell.jl#L240-L286">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TLSTM" href="#RecurrentLayers.TLSTM"><code>RecurrentLayers.TLSTM</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TLSTM(input_size =&gt; hidden_size;
    return_state = false, kwargs...)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed long short term memory</a>. See <a href="../cells/#RecurrentLayers.TLSTMCell"><code>TLSTMCell</code></a> for a layer that processes a single sequence.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>return_state</code>: Option to return the last state together with the output. Default is <code>false</code>.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t + \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    c_t &amp;= f_t \odot c_{t-1} + (1 - f_t) \odot z_t \\
    h_t &amp;= c_t \odot o_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">tlstm(inp, state)
tlstm(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the tlstm. It should be a vector of size <code>input_size x len</code> or a matrix of size <code>input_size x len x batch_size</code>.</li><li><code>state</code>: The hidden state of the TLSTM. If given, it is a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>New hidden states <code>new_states</code> as an array of size <code>hidden_size x len x batch_size</code>. When <code>return_state = true</code> it returns a tuple of the hidden states <code>new_states</code> and the last state of the iteration.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/4adecf130ef088ae7b0a57d9e650ca5417b94f86/src/cells/trnn_cell.jl#L403-L450">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../cells/">« Cells</a><a class="docs-footer-nextpage" href="../wrappers/">Wrappers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 18 February 2025 10:59">Tuesday 18 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
