var documenterSearchIndex = {"docs":
[{"location":"api/layers/janet/#JANET","page":"JANET","title":"JANET","text":"","category":"section"},{"location":"api/layers/janet/#RecurrentLayers.JANET","page":"JANET","title":"RecurrentLayers.JANET","text":"JANET(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nJust another network (van der Westhuizen and Lasenby, 2018). See JANETCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\nbeta_value: control over the input data flow. Default is 1.0.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = mathbfW^f_hh mathbfh(t-1) + mathbfW^f_ih\n        mathbfx(t) + mathbfb^f \n    tildemathbfc(t) = tanhleft( mathbfW^c_hh mathbfh(t-1) +\n        mathbfW^c_ih mathbfx(t) + mathbfb^c right) \n    mathbfc(t) = sigmaleft( mathbfs(t) right) odot mathbfc(t-1) +\n        left( 1 - sigmaleft( mathbfs(t) - beta right) right) odot\n        tildemathbfc(t) \n    mathbfh(t) = mathbfc(t)\nendaligned\n\nForward\n\njanet(inp, (state, cstate))\njanet(inp)\n\nArguments\n\ninp: The input to the janet. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the JANET.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/cfn/#CFN","page":"CFN","title":"CFN","text":"","category":"section"},{"location":"api/layers/cfn/#RecurrentLayers.CFN","page":"CFN","title":"RecurrentLayers.CFN","text":"CFN(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nChaos free network unit (Laurent and Brecht, 2017). See CFNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfh(t) = boldsymboltheta(t) odot tanhleft( mathbfh(t-1)\n        right) + boldsymboleta(t) odot tanhleft( mathbfW_ih\n        mathbfx(t) right) \n    boldsymboltheta(t) = sigmaleft( mathbfW^theta_hh\n        mathbfh(t-1) + mathbfW^theta_ih mathbfx(t) +\n        mathbfb^theta right) \n    boldsymboleta(t) = sigmaleft( mathbfW^eta_hh\n        mathbfh(t-1) + mathbfW^eta_ih mathbfx(t) +\n        mathbfb^eta right)\nendaligned\n\nForward\n\ncfn(inp, state)\ncfn(inp)\n\nArguments\n\ninp: The input to the cfn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the CFN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/janetcell/#JANETCell","page":"JANETCell","title":"JANETCell","text":"","category":"section"},{"location":"api/cells/janetcell/#RecurrentLayers.JANETCell","page":"JANETCell","title":"RecurrentLayers.JANETCell","text":"JANETCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, beta_value=1.0)\n\nJust another network unit (van der Westhuizen and Lasenby, 2018). See JANET for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nbeta_value: control over the input data flow. Default is 1.0.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = mathbfW^f_hh mathbfh(t-1) + mathbfW^f_ih\n        mathbfx(t) + mathbfb^f \n    tildemathbfc(t) = tanhleft( mathbfW^c_hh mathbfh(t-1) +\n        mathbfW^c_ih mathbfx(t) + mathbfb^c right) \n    mathbfc(t) = sigmaleft( mathbfs(t) right) odot mathbfc(t-1) +\n        left( 1 - sigmaleft( mathbfs(t) - beta right) right) odot\n        tildemathbfc(t) \n    mathbfh(t) = mathbfc(t)\nendaligned\n\nForward\n\njanetcell(inp, (state, cstate))\njanetcell(inp)\n\nArguments\n\ninp: The input to the rancell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/ligrucell/#LiGRUCell","page":"LiGRUCell","title":"LiGRUCell","text":"","category":"section"},{"location":"api/cells/ligrucell/#RecurrentLayers.LiGRUCell","page":"LiGRUCell","title":"RecurrentLayers.LiGRUCell","text":"LiGRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLight gated recurrent unit (Ravanelli et al., 2018). The implementation does not include the batch normalization as described in the original paper. See LiGRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    tildemathbfh(t) = textReLUleft( mathbfW^h_ih\n        mathbfx(t) + mathbfW^h_hh mathbfh(t-1) + mathbfb^h\n        right) \n    mathbfh(t) = mathbfz(t) odot mathbfh(t-1) + left(1 -\n        mathbfz(t)right) odot tildemathbfh(t)\nendaligned\n\nForward\n\nligrucell(inp, state)\nligrucell(inp)\n\nArguments\n\ninp: The input to the ligrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the LiGRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"roadmap/#Roadmap","page":"Roadmap","title":"Roadmap","text":"","category":"section"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"This page documents some planned work for RecurrentLayers.jl. Future work for this library includes additional cells such as:","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"FastRNNs and FastGRUs (current focus) arxiv\nUnitary recurrent neural networks arxiv\nModern recurrent neural networks such as LRU  and minLSTM/minGRU\nQuasi recurrent neural networks arxiv","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"Additionally, some cell-independent architectures are also planned, that expand the ability of recurrent architectures and could theoretically take any cell:","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"Clockwork rnns arxiv\nPhased rnns arxiv\nSegment rnn arxiv\nFast-Slow rnns arxiv","category":"page"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"An implementation of these ideally would be, for example FastSlow(RNNCell, input_size => hidden_size). More details on this soon!","category":"page"},{"location":"api/wrappers/#Wrappers","page":"Wrappers","title":"Wrappers","text":"","category":"section"},{"location":"api/wrappers/#Wrappers-for-layers","page":"Wrappers","title":"Wrappers for layers","text":"","category":"section"},{"location":"api/wrappers/","page":"Wrappers","title":"Wrappers","text":"StackedRNN","category":"page"},{"location":"api/wrappers/#Wrappers-for-cells","page":"Wrappers","title":"Wrappers for cells","text":"","category":"section"},{"location":"api/wrappers/","page":"Wrappers","title":"Wrappers","text":"Multiplicative: Multiplicative addition internal state","category":"page"},{"location":"api/cells/indrnncell/#IndRNNCell","page":"IndRNNCell","title":"IndRNNCell","text":"","category":"section"},{"location":"api/cells/indrnncell/#RecurrentLayers.IndRNNCell","page":"IndRNNCell","title":"RecurrentLayers.IndRNNCell","text":"IndRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nIndependently recurrent cell (Li et al., Jun 2018). See IndRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\n    mathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) + mathbfu\n        odot mathbfh(t-1) + mathbfb right)\n\nForward\n\nindrnncell(inp, state)\nindrnncell(inp)\n\nArguments\n\ninp: The input to the indrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the IndRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/multiplicativelstm/#MultiplicativeLSTM","page":"MultiplicativeLSTM","title":"MultiplicativeLSTM","text":"","category":"section"},{"location":"api/layers/multiplicativelstm/#RecurrentLayers.MultiplicativeLSTM","page":"MultiplicativeLSTM","title":"RecurrentLayers.MultiplicativeLSTM","text":"MultiplicativeLSTM(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nMultiplicative long short term memory network (Krause et al., 2017). See MultiplicativeLSTMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_multiplicative_kernel: initializer for the multiplicative to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfm(t) = left( mathbfW^m_ih mathbfx(t) right) circ\n        left( mathbfW^m_hh mathbfh(t-1) right) \n    hatmathbfh(t) = mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_mh mathbfm(t) + mathbfb^h \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_mh mathbfm(t) + mathbfb^i right) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_mh mathbfm(t) + mathbfb^o right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_mh mathbfm(t) + mathbfb^f right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t)\n        circ tanhleft( hatmathbfh(t) right) \n    mathbfh(t) = tanhleft( mathbfc(t) right) circ mathbfo(t)\nendaligned\n\nForward\n\nmultiplicativelstm(inp, (state, cstate))\nmultiplicativelstm(inp)\n\nArguments\n\ninp: The input to the multiplicativelstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the MultiplicativeLSTM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/tgru/#TGRU","page":"TGRU","title":"TGRU","text":"","category":"section"},{"location":"api/layers/tgru/#RecurrentLayers.TGRU","page":"TGRU","title":"RecurrentLayers.TGRU","text":"TGRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nStrongly typed recurrent gated unit (Balduzzi and Ghifary, 20–22 Jun 2016). See TGRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfx(t-1) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfx(t-1) + mathbfb^f right) \n    mathbfo(t) = tauleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfx(t-1) + mathbfb^o right) \n    mathbfh(t) = mathbff(t) odot mathbfh(t-1) +\n        mathbfz(t) odot mathbfo(t)\nendaligned\n\nForward\n\ntgru(inp, state)\ntgru(inp)\n\nArguments\n\ninp: The input to the tgru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TGRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cfncell/#CFNCell","page":"CFNCell","title":"CFNCell","text":"","category":"section"},{"location":"api/cells/cfncell/#RecurrentLayers.CFNCell","page":"CFNCell","title":"RecurrentLayers.CFNCell","text":"CFNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nChaos free network unit (Laurent and Brecht, 2017). See CFN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfh(t) = boldsymboltheta(t) odot tanhleft( mathbfh(t-1)\n        right) + boldsymboleta(t) odot tanhleft( mathbfW_ih\n        mathbfx(t) right) \n    boldsymboltheta(t) = sigmaleft( mathbfW^theta_hh\n        mathbfh(t-1) + mathbfW^theta_ih mathbfx(t) +\n        mathbfb^theta right) \n    boldsymboleta(t) = sigmaleft( mathbfW^eta_hh\n        mathbfh(t-1) + mathbfW^eta_ih mathbfx(t) +\n        mathbfb^eta right)\nendaligned\n\nForward\n\ncfncell(inp, state)\ncfncell(inp)\n\nArguments\n\ninp: The input to the cfncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the CFNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/multiplicativelstmcell/#MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"MultiplicativeLSTMCell","text":"","category":"section"},{"location":"api/cells/multiplicativelstmcell/#RecurrentLayers.MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"RecurrentLayers.MultiplicativeLSTMCell","text":"MultiplicativeLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    init_multiplicative_kernel=glorot_uniform,\n    bias = true)\n\nMultiplicative long short term memory cell (Krause et al., 2017). See MultiplicativeLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_multiplicative_kernel: initializer for the multiplicative to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfm(t) = left( mathbfW^m_ih mathbfx(t) right) circ\n        left( mathbfW^m_hh mathbfh(t-1) right) \n    hatmathbfh(t) = mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_mh mathbfm(t) + mathbfb^h \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_mh mathbfm(t) + mathbfb^i right) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_mh mathbfm(t) + mathbfb^o right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_mh mathbfm(t) + mathbfb^f right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t)\n        circ tanhleft( hatmathbfh(t) right) \n    mathbfh(t) = tanhleft( mathbfc(t) right) circ mathbfo(t)\nendaligned\n\nForward\n\nmultiplicativelstmcell(inp, (state, cstate))\nmultiplicativelstmcell(inp)\n\nArguments\n\ninp: The input to the multiplicativelstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the MultiplicativeLSTMCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/minimalrnn/#MinimalRNN","page":"MinimalRNN","title":"MinimalRNN","text":"","category":"section"},{"location":"api/layers/minimalrnn/#RecurrentLayers.MinimalRNN","page":"MinimalRNN","title":"RecurrentLayers.MinimalRNN","text":"MinimalRNN(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nMinimal recurrent neural network (Chen, 2017). See MinimalRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nencoder_bias: include a bias in the encoder or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = Phi(mathbfx(t)) = tanhleft( mathbfW_xz\n        mathbfx(t) + mathbfb^z right) \n    mathbfu(t) = sigmaleft( mathbfW_hh^u mathbfh(t-1) +\n        mathbfW_zh^u mathbfz(t) + mathbfb^u right) \n    mathbfh(t) = mathbfu(t) circ mathbfh(t-1) + left(1 -\n        mathbfu(t)right) circ mathbfz(t)\nendaligned\n\nForward\n\nminimalrnn(inp, (state, c_state))\nminimalrnn(inp)\n\nArguments\n\ninp: The input to the minimalrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the MinimalRNN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/wmclstm/#WMCLSTM","page":"WMCLSTM","title":"WMCLSTM","text":"","category":"section"},{"location":"api/layers/wmclstm/#RecurrentLayers.WMCLSTM","page":"WMCLSTM","title":"RecurrentLayers.WMCLSTM","text":"WMCLSTM(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nLong short term memory cell with working memory connections (Landi et al., 2021). See WMCLSTM for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_memory_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) +\n        tanhleft( mathbfW^i_ch mathbfc(t-1) right) +\n        mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) +\n        tanhleft( mathbfW^f_ch mathbfc(t-1) right) +\n        mathbfb^f right) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfh(t-1) +\n        tanhleft( mathbfW^o_ch mathbfc(t) right) +\n        mathbfb^o right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t) circ\n        sigma_cleft( mathbfW^c_ih mathbfx(t) + mathbfb^c right) \n    mathbfh(t) = mathbfo(t) circ sigma_hleft( mathbfc(t) right)\nendaligned\n\nForward\n\nwmclstm(inp, (state, cstate))\nwmclstm(inp)\n\nArguments\n\ninp: The input to the wmclstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the WMCLSTM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/rhncell/#RHNCell","page":"RHNCell","title":"RHNCell","text":"","category":"section"},{"location":"api/cells/rhncell/#RecurrentLayers.RHNCell","page":"RHNCell","title":"RecurrentLayers.RHNCell","text":"RHNCell(input_size => hidden_size, [depth];\n    couple_carry = true,\n    cell_kwargs...)\n\nRecurrent highway network (Zilly et al., 06–11 Aug 2017). See RHN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndepth: depth of the recurrence. Default is 3.\n\nKeyword arguments\n\ncouple_carry: couples the carry gate and the transform gate. Default true\ninit_kernel: initializer for the input to hidden weights. Default is glorot_uniform\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    mathbfs_ell(t) = mathbfh_ell(t) odot mathbft_ell(t) +\n        mathbfs_ell-1(t) odot mathbfc_ell(t) \n    mathbfh_ell(t) = tanhleft( mathbfW^h_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^h_ell_hh mathbfs_ell-1(t)\n        + mathbfb^h_ell right) \n    mathbft_ell(t) = sigmaleft( mathbfW^t_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^t_ell_hh mathbfs_ell-1(t)\n        + mathbfb^t_ell right) \n    mathbfc_ell(t) = sigmaleft( mathbfW^c_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^c_ell_hh mathbfs_ell-1(t)\n        + mathbfb^c_ell right)\nendaligned\n\nForward\n\nrnncell(inp, [state])\n\n\n\n\n\n","category":"type"},{"location":"api/layers/peepholelstm/#PeepholeLSTM","page":"PeepholeLSTM","title":"PeepholeLSTM","text":"","category":"section"},{"location":"api/layers/peepholelstm/#RecurrentLayers.PeepholeLSTM","page":"PeepholeLSTM","title":"RecurrentLayers.PeepholeLSTM","text":"PeepholeLSTM(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nPeephole long short term memory network (Gers et al., 2002). See PeepholeLSTMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) + mathbfw^i_ph odot\n        mathbfc(t-1) + mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfw^f_ph odot\n        mathbfc(t-1) + mathbfb^f right) \n    mathbfc(t) = mathbff(t) odot mathbfc(t-1) + mathbfi(t)\n        odot mathbfz(t) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfh(t-1) + mathbfw^o_ph odot\n        mathbfc(t) + mathbfb^o right) \n    mathbfh(t) = mathbfo(t) odot tanhleft( mathbfc(t) right)\nendaligned\n\nForward\n\npeepholelstm(inp, (state, cstate))\npeepholelstm(inp)\n\nArguments\n\ninp: The input to the peepholelstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the PeepholeLSTM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mgucell/#MGUCell","page":"MGUCell","title":"MGUCell","text":"","category":"section"},{"location":"api/cells/mgucell/#RecurrentLayers.MGUCell","page":"MGUCell","title":"RecurrentLayers.MGUCell","text":"MGUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMinimal gated unit (Zhou et al., 2016). See MGU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfb^f right) \n    tildemathbfh(t) = tanhleft( mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_hh left( mathbff(t) odot mathbfh(t-1) right) +\n        mathbfb^h right) \n    mathbfh(t) = left(1 - mathbff(t)right) odot mathbfh(t-1) +\n        mathbff(t) odot tildemathbfh(t)\nendaligned\n\nForward\n\nmgucell(inp, state)\nmgucell(inp)\n\nArguments\n\ninp: The input to the mgucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MGUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/mut1/#MUT1","page":"MUT1","title":"MUT1","text":"","category":"section"},{"location":"api/layers/mut1/#RecurrentLayers.MUT1","page":"MUT1","title":"RecurrentLayers.MUT1","text":"MUT1(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nMutated unit 1 network (Jozefowicz et al., 07–09 Jul 2015). See MUT1Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfb^z right) \n    mathbfr(t) = sigmaleft( mathbfW^r_ih mathbfx(t) +\n        mathbfW^r_hh mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + tanhleft(\n        mathbfW^h_ih mathbfx(t) + mathbfb^h right) +\n        mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/atrcell/#ATRCell","page":"ATRCell","title":"ATRCell","text":"","category":"section"},{"location":"api/cells/atrcell/#RecurrentLayers.ATRCell","page":"ATRCell","title":"RecurrentLayers.ATRCell","text":"ATRCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform)\n\nAddition-subtraction twin-gated recurrent cell (Zhang et al., 2018). See ATR for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfp(t) = mathbfW_ih mathbfx(t) + mathbfb \n    mathbfq(t) = mathbfW_hh mathbfh(t-1) \n    mathbfi(t) = sigmaleft( mathbfp(t) + mathbfq(t) right) \n    mathbff(t) = sigmaleft( mathbfp(t) - mathbfq(t) right) \n    mathbfh(t) = mathbfi(t) circ mathbfp(t) + mathbff(t) circ\n        mathbfh(t-1)\nendaligned\n\n\nForward\n\natrcell(inp, state)\natrcell(inp)\n\nArguments\n\ninp: The input to the atrcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the ATRCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/mut3/#MUT3","page":"MUT3","title":"MUT3","text":"","category":"section"},{"location":"api/layers/mut3/#RecurrentLayers.MUT3","page":"MUT3","title":"RecurrentLayers.MUT3","text":"MUT3(input_size => hidden_size;\nreturn_state = false, kwargs...)\n\nMutated unit 3 network (Jozefowicz et al., 07–09 Jul 2015). See MUT3Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh tanhleft( mathbfh(t) right) + mathbfb^z\n        right) \n    mathbfr(t) = sigmaleft( mathbfW^r_ih mathbfx(t) +\n        mathbfW^r_hh mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + mathbfW^h_ih\n        mathbfx(t) + mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/lem/#LEM","page":"LEM","title":"LEM","text":"","category":"section"},{"location":"api/layers/lem/#RecurrentLayers.LEM","page":"LEM","title":"RecurrentLayers.LEM","text":"LEM(input_size => hidden_size, [dt];\n    return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nLong expressive memory network (Rusch et al., 2022). See LEMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: timestep. Defaul is 1.0.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    boldsymbolDelta t(t) = Delta hatt  hatsigma left(\n        mathbfW^1_hh mathbfh(t-1) + mathbfW^1_ih\n        mathbfx(t) + mathbfb^1 right) \n    overlineboldsymbolDelta t(t) = Delta hatt  hatsigma\n        left( mathbfW^2_hh mathbfh(t-1) + mathbfW^2_ih\n        mathbfx(t) + mathbfb^2 right) \n    mathbfz(t) = left( 1 - boldsymbolDelta t(t) right) odot\n        mathbfz(t-1) + boldsymbolDelta t(t) odot sigma left(\n        mathbfW^z_hh mathbfh(t-1) + mathbfW^z_ih mathbfx(t)\n        + mathbfb^z right) \n    mathbfh(t) = left( 1 - boldsymbolDelta t(t) right) odot\n        mathbfh(t-1) + boldsymbolDelta t(t) odot sigma left(\n        mathbfW^h_zh mathbfz(t) + mathbfW^h_ih mathbfx(t) +\n        mathbfb^h right)\nendaligned\n\nForward\n\nlem(inp, (state, zstate))\nlem(inp)\n\nArguments\n\ninp: The input to the LEM. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the LEM.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/lightru/#LightRU","page":"LightRU","title":"LightRU","text":"","category":"section"},{"location":"api/layers/lightru/#RecurrentLayers.LightRU","page":"LightRU","title":"RecurrentLayers.LightRU","text":"LightRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nLight recurrent unit network (Ye et al., 2024). See LightRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = tanhleft( mathbfW_ih^h mathbfx(t) right) \n    mathbff(t) = deltaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb^f right) \n    mathbfh(t) = left( 1 - mathbff(t) right) odot mathbfh(t-1) +\n        mathbff(t) odot tildemathbfh(t)\nendaligned\n\nForward\n\nlightru(inp, state)\nlightru(inp)\n\nArguments\n\ninp: The input to the lightru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the LightRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cornncell/#coRNNCell","page":"coRNNCell","title":"coRNNCell","text":"","category":"section"},{"location":"api/cells/cornncell/#RecurrentLayers.coRNNCell","page":"coRNNCell","title":"RecurrentLayers.coRNNCell","text":"coRNNCell(input_size => hidden_size, [dt];\n    gamma=0.0, epsilon=0.0,\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nCoupled oscillatory recurrent neural unit (Rusch and Mishra, 2021). See coRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\ngamma: damping for state. Default is 0.0.\nepsilon: damping for candidate state. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfz(t-1) + Delta t  sigma left( mathbfW_hh\n        mathbfh(t-1) + mathbfW_zh mathbfz(t-1) + mathbfW_ih\n        mathbfx(t) + mathbfb right) - Delta t  gamma mathbfh(t-1)\n        - Delta t  epsilon mathbfz(t) \n    mathbfh(t) = mathbfh(t-1) + Delta t  mathbfz(t)\nendaligned\n\nForward\n\ncornncell(inp, (state, cstate))\ncornncell(inp)\n\nArguments\n\ninp: The input to the cornncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the coRNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastrnncell/#FastRNNCell","page":"FastRNNCell","title":"FastRNNCell","text":"","category":"section"},{"location":"api/cells/fastrnncell/#RecurrentLayers.FastRNNCell","page":"FastRNNCell","title":"RecurrentLayers.FastRNNCell","text":"FastRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    init_alpha = 3.0, init_beta = - 3.0,\n    bias = true)\n\nFast recurrent neural network cell (Kusupati et al., 2018). See FastRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_alpha: Initializer for the alpha parameter. Default is 3.0.\ninit_beta: Initializer for the beta parameter. Default is - 3.0.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfW_hh mathbfh(t-1) + mathbfb right) \n    mathbfh(t) = alpha  tildemathbfh(t) + beta  mathbfh(t-1)\nendaligned\n\nForward\n\nfastrnncell(inp, state)\nfastrnncell(inp)\n\nArguments\n\ninp: The input to the fastrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/cornn/#coRNN","page":"coRNN","title":"coRNN","text":"","category":"section"},{"location":"api/layers/cornn/#RecurrentLayers.coRNN","page":"coRNN","title":"RecurrentLayers.coRNN","text":"coRNN(input_size => hidden_size, [dt];\n    gamma=0.0, epsilon=0.0,\n    return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nCoupled oscillatory recurrent neural unit (Rusch and Mishra, 2021). See coRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\ngamma: damping for state. Default is 0.0.\nepsilon: damping for candidate state. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfz(t-1) + Delta t  sigma left( mathbfW_hh\n        mathbfh(t-1) + mathbfW_zh mathbfz(t-1) + mathbfW_ih\n        mathbfx(t) + mathbfb right) - Delta t  gamma mathbfh(t-1)\n        - Delta t  epsilon mathbfz(t) \n    mathbfh(t) = mathbfh(t-1) + Delta t  mathbfz(t)\nendaligned\n\nForward\n\ncornn(inp, (state, zstate))\ncornn(inp)\n\nArguments\n\ninp: The input to the cornn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the coRNN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"AntisymmetricRNN: Antisymmetric Recurrent Neural Network\nATR: Addition-Subtraction Twin-Gated Recurrent Network\nBR: Bistable Recurrent Network\nCFN: Chaos Free Network\ncoRNN: Coupled Oscillatory Recurrent Neural Network\nFastGRNN: Fast Gated Recurrent Neural Network\nFastRNN: Fast Recurrent Neural Network\nFSRNN: FastSlow Recurrent Neural Network\nGatedAntisymmetricRNN: Gated Antisymmetric Recurrent Neural Network\nIndRNN: Independently Recurrent Neural Network\nJANET: Just Another Network\nLEM: Long Expressive Memory Network\nLiGRU: Light Gated Recurrent Unit\nLightRU: Light Recurrent Unit\nMGU: Minimal Gated Unit\nMinimalRNN: Minimal Recurrent Neural Network\nMultiplicativeLSTM: Multiplicative Long Short-Term Memory\nMUT1: Mutated RNN Variant 1\nMUT2: Mutated RNN Variant 2\nMUT3: Mutated RNN Variant 3\nNAS: Neural Architecture Search Network\nNBR: Neuromodulated Bistable Recurrent Network\nPeepholeLSTM: Peephole Long Short-Term Memory\nRAN: Recurrent Additive Network\nRHN: Recurrent Highway Network\nSCRN: Structurally Constrained Recurrent Network\nSGRN: Simple Gated Recurrent Network\nSTAR: Stackable Recurrent Network\nTGRU: Typed Gated Recurrent Unit\nTLSTM: Typed Long Short-Term Memory\nTRNN: Typed Recurrent Neural Network\nUnICORNN: Undamped Independent Controlled Oscillatory Recurrent Neural Network\nWMCLSTM: Working Memory Connection Long Short-Term Memory","category":"page"},{"location":"api/cells/minimalrnncell/#MinimalRNNCell","page":"MinimalRNNCell","title":"MinimalRNNCell","text":"","category":"section"},{"location":"api/cells/minimalrnncell/#RecurrentLayers.MinimalRNNCell","page":"MinimalRNNCell","title":"RecurrentLayers.MinimalRNNCell","text":"MinimalRNNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, encoder_bias = true)\n\nMinimal recurrent neural network unit (Chen, 2017). See MinimalRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nencoder_bias: include a bias in the encoder or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = Phi(mathbfx(t)) = tanhleft( mathbfW_xz\n        mathbfx(t) + mathbfb^z right) \n    mathbfu(t) = sigmaleft( mathbfW_hh^u mathbfh(t-1) +\n        mathbfW_zh^u mathbfz(t) + mathbfb^u right) \n    mathbfh(t) = mathbfu(t) circ mathbfh(t-1) + left(1 -\n        mathbfu(t)right) circ mathbfz(t)\nendaligned\n\nForward\n\nminimalrnncell(inp, state)\nminimalrnncell(inp)\n\nArguments\n\ninp: The input to the minimalrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the MinimalRNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/nbr/#NBR","page":"NBR","title":"NBR","text":"","category":"section"},{"location":"api/layers/nbr/#RecurrentLayers.NBR","page":"NBR","title":"RecurrentLayers.NBR","text":"NBR(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nRecurrently neuromodulated bistable recurrent cell (Vecoven et al., 2021). See NBRCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft( mathbfW^a_ih mathbfx(t) +\n        mathbfW^a_hh mathbfh(t-1) + mathbfb^a right) \n    mathbfc(t) = sigmaleft( mathbfW^c_ih mathbfx(t) +\n        mathbfW^c_hh mathbfh(t-1) + mathbfb^c right) \n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + left(1 -\n        mathbfc(t)right) circ tanhleft( mathbfW^h_ih\n        mathbfx(t) + mathbfa(t) circ mathbfh(t-1) + mathbfb^h right)\nendaligned\n\nForward\n\nnbr(inp, state)\nnbr(inp)\n\nArguments\n\ninp: The input to the nbr. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the NBR. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fsrnncell/#FSRNNCell","page":"FSRNNCell","title":"FSRNNCell","text":"","category":"section"},{"location":"api/cells/fsrnncell/#RecurrentLayers.FSRNNCell","page":"FSRNNCell","title":"RecurrentLayers.FSRNNCell","text":"FSRNNCell(input_size => hidden_size,\n    fast_cells, slow_cell)\n\nFast slow recurrent neural network cell (Mujika et al., 2017). See FSRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\nfast_cells: a vector of the fast cells. Must be minimum of length 2.\nslow_cell: the chosen slow cell.\n\nEquations\n\nbeginaligned\n    mathbfh^F_1(t) = f^F_1left( mathbfh^F_k(t-1) mathbfx(t)\n        right) \n    mathbfh^S(t) = f^Sleft( mathbfh^S(t-1) mathbfh^F_1(t)\n        right) \n    mathbfh^F_2(t) = f^F_2left( mathbfh^F_1(t) mathbfh^S(t)\n        right) \n    mathbfh^F_i(t) = f^F_ileft( mathbfh^F_i-1(t) right) quad\n        textfor  3 leq i leq k\nendaligned\n\nForward\n\nfsrnncell(inp, (fast_state, slow_state))\nfsrnncell(inp)\n\nArguments\n\ninp: The input to the fsrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(fast_state, slow_state): A tuple containing the hidden and cell states of the FSRNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (fast_state, slow_state) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/star/#STAR","page":"STAR","title":"STAR","text":"","category":"section"},{"location":"api/layers/star/#RecurrentLayers.STAR","page":"STAR","title":"RecurrentLayers.STAR","text":"STAR(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nStackable recurrent network (Turkoglu et al., 2021). See STARCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW^z_ih mathbfx(t) +\n        mathbfb^z right) \n    mathbfk(t) = sigmaleft( mathbfW^k_ih mathbfx(t) +\n        mathbfW^k_hh mathbfh(t-1) + mathbfb^k right) \n    mathbfh(t) = tanhleft( left(1 - mathbfk(t)right) circ\n        mathbfh(t-1) + mathbfk(t) circ mathbfz(t) right)\nendaligned\n\nForward\n\nstar(inp, state)\nstar(inp)\n\nArguments\n\ninp: The input to the star. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the STAR. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/antisymmetricrnn/#AntisymmetricRNN","page":"AntisymmetricRNN","title":"AntisymmetricRNN","text":"","category":"section"},{"location":"api/layers/antisymmetricrnn/#RecurrentLayers.AntisymmetricRNN","page":"AntisymmetricRNN","title":"RecurrentLayers.AntisymmetricRNN","text":"AntisymmetricRNN(input_size, hidden_size, [activation];\n    return_state = false, kwargs...)\n\nAntisymmetric recurrent neural network (Chang et al., 2019). See AntisymmetricRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nEquations\n\n    mathbfh(t) = mathbfh(t-1) + epsilon tanh left( mathbfW_ih\n        mathbfx(t) + left( mathbfW_hh - mathbfW_hh^top - gamma\n        mathbfI right) mathbfh(t-1) + mathbfb right)\n\nForward\n\nasymrnn(inp, state)\nasymrnn(inp)\n\nArguments\n\ninp: The input to the asymrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the AntisymmetricRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/lemcell/#LEMCell","page":"LEMCell","title":"LEMCell","text":"","category":"section"},{"location":"api/cells/lemcell/#RecurrentLayers.LEMCell","page":"LEMCell","title":"RecurrentLayers.LEMCell","text":"LEMCell(input_size => hidden_size, [dt];\n    init_kernel = glorot_uniform, init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLong expressive memory unit (Rusch et al., 2022). See LEM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: timestep. Defaul is 1.0.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    boldsymbolDelta t(t) = Delta hatt  hatsigma left(\n        mathbfW^1_hh mathbfh(t-1) + mathbfW^1_ih\n        mathbfx(t) + mathbfb^1 right) \n    overlineboldsymbolDelta t(t) = Delta hatt  hatsigma\n        left( mathbfW^2_hh mathbfh(t-1) + mathbfW^2_ih\n        mathbfx(t) + mathbfb^2 right) \n    mathbfz(t) = left( 1 - boldsymbolDelta t(t) right) odot\n        mathbfz(t-1) + boldsymbolDelta t(t) odot sigma left(\n        mathbfW^z_hh mathbfh(t-1) + mathbfW^z_ih mathbfx(t)\n        + mathbfb^z right) \n    mathbfh(t) = left( 1 - boldsymbolDelta t(t) right) odot\n        mathbfh(t-1) + boldsymbolDelta t(t) odot sigma left(\n        mathbfW^h_zh mathbfz(t) + mathbfW^h_ih mathbfx(t) +\n        mathbfb^h right)\nendaligned\n\nForward\n\nlemcell(inp, (state, cstate))\nlemcell(inp)\n\nArguments\n\ninp: The input to the lemcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nbrcell/#NBRCell","page":"NBRCell","title":"NBRCell","text":"","category":"section"},{"location":"api/cells/nbrcell/#RecurrentLayers.NBRCell","page":"NBRCell","title":"RecurrentLayers.NBRCell","text":"NBRCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform)\n\nRecurrently neuromodulated bistable recurrent cell (Vecoven et al., 2021). See NBR for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft( mathbfW^a_ih mathbfx(t) +\n        mathbfW^a_hh mathbfh(t-1) + mathbfb^a right) \n    mathbfc(t) = sigmaleft( mathbfW^c_ih mathbfx(t) +\n        mathbfW^c_hh mathbfh(t-1) + mathbfb^c right) \n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + left(1 -\n        mathbfc(t)right) circ tanhleft( mathbfW^h_ih\n        mathbfx(t) + mathbfa(t) circ mathbfh(t-1) + mathbfb^h right)\nendaligned\n\nForward\n\nnbrcell(inp, state)\nnbrcell(inp)\n\nArguments\n\ninp: The input to the nbrcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the NBRCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nascell/#NASCell","page":"NASCell","title":"NASCell","text":"","category":"section"},{"location":"api/cells/nascell/#RecurrentLayers.NASCell","page":"NASCell","title":"RecurrentLayers.NASCell","text":"NASCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nNeural Architecture Search unit (Zoph and Le, 2017). See NAS for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfo_1 = sigmaleft( mathbfW^(1)_ih mathbfx(t) +\n        mathbfW^(1)_hh mathbfh(t-1) + mathbfb^(1) right) \n    mathbfo_2 = textReLUleft( mathbfW^(2)_ih mathbfx(t) +\n        mathbfW^(2)_hh mathbfh(t-1) + mathbfb^(2) right) \n    mathbfo_3 = sigmaleft( mathbfW^(3)_ih mathbfx(t) +\n        mathbfW^(3)_hh mathbfh(t-1) + mathbfb^(3) right) \n    mathbfo_4 = textReLUleft( mathbfW^(4)_ih mathbfx(t)\n        cdot mathbfW^(4)_hh mathbfh(t-1) right) \n    mathbfo_5 = tanhleft( mathbfW^(5)_ih mathbfx(t) +\n        mathbfW^(5)_hh mathbfh(t-1) + mathbfb^(5) right) \n    mathbfo_6 = sigmaleft( mathbfW^(6)_ih mathbfx(t) +\n        mathbfW^(6)_hh mathbfh(t-1) + mathbfb^(6) right) \n    mathbfo_7 = tanhleft( mathbfW^(7)_ih mathbfx(t) +\n        mathbfW^(7)_hh mathbfh(t-1) + mathbfb^(7) right) \n    mathbfo_8 = sigmaleft( mathbfW^(8)_ih mathbfx(t) +\n        mathbfW^(8)_hh mathbfh(t-1) + mathbfb^(8) right) 1ex\n    mathbfl_1 = tanhleft( mathbfo_1 cdot mathbfo_2 right) \n    mathbfl_2 = tanhleft( mathbfo_3 + mathbfo_4 right) \n    mathbfl_3 = tanhleft( mathbfo_5 cdot mathbfo_6 right) \n    mathbfl_4 = sigmaleft( mathbfo_7 + mathbfo_8 right) 1ex\n    mathbfl_1 = tanhleft( mathbfl_1 + mathbfc_textstate\n        right) 1ex\n    mathbfc_textnew = mathbfl_1 cdot mathbfl_2 \n    mathbfl_5 = tanhleft( mathbfl_3 + mathbfl_4 right) \n    mathbfh_textnew = tanhleft( mathbfc_textnew cdot\n        mathbfl_5 right)\nendaligned\n\nForward\n\nnascell(inp, (state, cstate))\nnascell(inp)\n\nArguments\n\ninp: The input to the nascell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the NASCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"#RecurrentLayers","page":"Home","title":"RecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RecurrentLayers.jl extends Flux.jl recurrent layers offering by providing implementations of additional recurrent layers not available in base deep learning libraries.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package offers multiple layers for Flux.jl. Currently there are 30+ cells implemented, together with multiple higher level implementations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Short name Publication venue Official implementation\nAntisymmetricRNN/GatedAntisymmetricRNN ICLR 2019 –\nATR EMNLP 2018 bzhangGo/ATR\nBR/BRC PLOS ONE 2021 nvecoven/BRC\nCFN ICLR 2017 –\ncoRNN ICLR 2021 tk-rusch/coRNN\nFastRNN/FastGRNN NeurIPS 2018 Microsoft/EdgeML\nFSRNN NeurIPS 2017 amujika/Fast-Slow-LSTM\nIndRNN CVPR 2018 Sunnydreamrain/IndRNNTheanoLasagne\nJANET arXiv 2018 JosvanderWesthuizen/janet\nLEM ICLR 2022 tk-rusch/LEM\nLiGRU IEEE Transactions on Emerging Topics in Computing 2018 mravanelli/theano-kaldi-rnn\nLightRU MDPI Electronics 2023 –\nMinimalRNN NeurIPS 2017 –\nMultiplicativeLSTM Workshop ICLR 2017 benkrause/mLSTM\nMGU International Journal of Automation and Computing 2016 –\nMUT1/MUT2/MUT3 ICML 2015 –\nNAS arXiv 2016 tensorflow_addons/rnn\nPeepholeLSTM JMLR 2002 –\nRAN arXiv 2017 kentonl/ran\nRHN ICML 2017 jzilly/RecurrentHighwayNetworks\nSCRN ICLR 2015 facebookarchive/SCRNNs\nSGRN IET 2018 –\nSTAR IEEE Transactions on Pattern Analysis and Machine Intelligence 2022 0zgur0/STAckable-Recurrent-network\nTyped RNN / GRU / LSTM ICML 2016 –\nUnICORNN ICML 2021 tk-rusch/unicornn\nWMCLSTM Neural Networks 2021 –","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additional wrappers: Stacked RNNs, and Multiplicative RNN.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install RecurrentLayers using either of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RecurrentLayers\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add RecurrentLayers","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are always welcome! We specifically look for :","category":"page"},{"location":"","page":"Home","title":"Home","text":"Recurrent cells you would like to see implemented \nBenchmarks\nFixes for any bugs/errors\nDocumentation, in any form: examples, how tos, docstrings","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please consider the following guidelines before opening a pull request:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The code should be formatted according to the format file provided\nVariable names should be meaningful: please no single letter variables, and try to avoid double letters variables too. I know at the moment there are some in the codebase, but I will need a breaking change in order to fix the majority of them.\nThe format file does not format markdown. If you are adding docs, or docstrings please take care of not going over 92 cols.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For any clarification feel free to contact me directly (@MartinuzziFrancesco) either in the julia slack, by email or X/bluesky.","category":"page"},{"location":"references/#Bibliography","page":"References","title":"Bibliography","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Balduzzi, D. and Ghifary, M. (20–22 Jun 2016). Strongly-Typed Recurrent Neural Networks. In: Proceedings of The 33rd International Conference on Machine Learning, Vol. 48 of Proceedings of Machine Learning Research, edited by Balcan, M. F. and Weinberger, K. Q. (PMLR, New York, New York, USA); pp. 1292–1300.\n\n\n\nChang, B.; Chen, M.; Haber, E. and Chi, E. H. (2019). AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks. In: International Conference on Learning Representations.\n\n\n\nChen, M. (2017). MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks.\n\n\n\nGers, F. A.; Schraudolph, N. N. and Schmidhuber, J. (2002). Learning precise timing with LSTM recurrent networks. Journal of Machine Learning Research 3, 115–143.\n\n\n\nJozefowicz, R.; Zaremba, W. and Sutskever, I. (07–09 Jul 2015). An Empirical Exploration of Recurrent Network Architectures. In: Proceedings of the 32nd International Conference on Machine Learning, Vol. 37 of Proceedings of Machine Learning Research, edited by Bach, F. and Blei, D. (PMLR, Lille, France); pp. 2342–2350.\n\n\n\nKrause, B.; Murray, I.; Renals, S. and Lu, L. (2017). Multiplicative LSTM for sequence modelling.\n\n\n\nKusupati, A.; Singh, M.; Bhatia, K.; Kumar, A.; Jain, P. and Varma, M. (2018). FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network. In: Advances in Neural Information Processing Systems, Vol. 31, edited by Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N. and Garnett, R. (Curran Associates, Inc.).\n\n\n\nLandi, F.; Baraldi, L.; Cornia, M. and Cucchiara, R. (2021). Working Memory Connections for LSTM. Neural Networks 144, 334–341.\n\n\n\nLaurent, T. and Brecht, J. (2017). A recurrent neural network without chaos. In: International Conference on Learning Representations.\n\n\n\nLee, K.; Levy, O. and Zettlemoyer, L. (2017). Recurrent Additive Networks, arXiv:1705.07393.\n\n\n\nLi, S.; Li, W.; Cook, C.; Zhu, C. and Gao, Y. (Jun 2018). Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (IEEE); pp. 5457–5466.\n\n\n\nMikolov, T.; Joulin, A.; Chopra, S.; Mathieu, M. and Ranzato, M. (2014). Learning longer memory in recurrent neural networks, arXiv preprint arXiv:1412.7753.\n\n\n\nMujika, A.; Meier, F. and Steger, A. (2017). Fast-Slow Recurrent Neural Networks. In: Advances in Neural Information Processing Systems, Vol. 30, edited by Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S. and Garnett, R. (Curran Associates, Inc.).\n\n\n\nRavanelli, M.; Brakel, P.; Omologo, M. and Bengio, Y. (2018). Light Gated Recurrent Units for Speech Recognition. IEEE Transactions on Emerging Topics in Computational Intelligence 2, 92–102.\n\n\n\nRusch, T. K. and Mishra, S. (2021). Coupled Oscillatory Recurrent Neural Network (co{RNN}): An accurate and (gradient) stable architecture for learning long time dependencies. In: International Conference on Learning Representations.\n\n\n\nRusch, T. K.; Mishra, S.; Erichson, N. B. and Mahoney, M. W. (2022). Long Expressive Memory for Sequence Modeling. In: International Conference on Learning Representations.\n\n\n\nRusch, T. K. and Mishra, S. (18–24 Jul 2021). UnICORNN: A recurrent model for learning very long time dependencies. In: Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, edited by Meila, M. and Zhang, T. (PMLR); pp. 9168–9178.\n\n\n\nSutskever, I.; Martens, J. and Hinton, G. E. (2011). Generating Text with Recurrent Neural Networks. In: Proceedings of the 28th International Conference on Machine Learning (ICML); pp. 1017–1024.\n\n\n\nTurkoglu, M. O.; D’Aronco, S.; Wegner, J. and Schindler, K. (2021). Gating Revisited: Deep Multi-layer RNNs That Can Be Trained. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1.\n\n\n\nVecoven, N.; Ernst, D. and Drion, G. (2021). A bio-inspired bistable recurrent cell allows for long-lasting memory. PLOS ONE 16, e0252676.\n\n\n\nvan der Westhuizen, J. and Lasenby, J. (2018). The unreasonable effectiveness of the forget gate.\n\n\n\nYe, H.; Zhang, Y.; Liu, H.; Li, X.; Chang, J. and Zheng, H. (2024). Light Recurrent Unit: Towards an Interpretable Recurrent Neural Network for Modeling Long-Range Dependency. Electronics 13, 3204.\n\n\n\nZhang, B.; Xiong, D.; Su, J.; Lin, Q. and Zhang, H. (2018). Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (Association for Computational Linguistics); pp. 4273–4283.\n\n\n\nZhou, G.-B.; Wu, J.; Zhang, C.-L. and Zhou, Z.-H. (2016). Minimal gated unit for recurrent neural networks. International Journal of Automation and Computing 13, 226–234.\n\n\n\nZilly, J. G.; Srivastava, R. K.; Koutnı́k, J. and Schmidhuber, J. (06–11 Aug 2017). Recurrent Highway Networks. In: Proceedings of the 34th International Conference on Machine Learning, Vol. 70 of Proceedings of Machine Learning Research, edited by Precup, D. and Teh, Y. W. (PMLR); pp. 4189–4198.\n\n\n\nZoph, B. and Le, Q. (2017). Neural Architecture Search with Reinforcement Learning. In: International Conference on Learning Representations.\n\n\n\nZu, X. and Wei, K. (2020). A simple gated recurrent network for detection of power quality disturbances. IET Generation,  Transmission &amp; Distribution 15, 751–761.\n\n\n\n","category":"page"},{"location":"api/cells/tlstmcell/#TLSTMCell","page":"TLSTMCell","title":"TLSTMCell","text":"","category":"section"},{"location":"api/cells/tlstmcell/#RecurrentLayers.TLSTMCell","page":"TLSTMCell","title":"RecurrentLayers.TLSTMCell","text":"TLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed long short term memory cell (Balduzzi and Ghifary, 20–22 Jun 2016). See TLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfx(t-1) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfx(t-1) + mathbfb^f right) \n    mathbfo(t) = tauleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfx(t-1) + mathbfb^o right) \n    mathbfc(t) = mathbff(t) odot mathbfc(t-1) +\n        left(1 - mathbff(t)right) odot mathbfz(t) \n    mathbfh(t) = mathbfc(t) odot mathbfo(t)\nendaligned\n\nForward\n\ntlstmcell(inp, state)\ntlstmcell(inp)\n\nArguments\n\ninp: The input to the tlstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TLSTMCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate, inp) is the new hidden and cell state, together with the current input.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/mgu/#MGU","page":"MGU","title":"MGU","text":"","category":"section"},{"location":"api/layers/mgu/#RecurrentLayers.MGU","page":"MGU","title":"RecurrentLayers.MGU","text":"MGU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nMinimal gated unit network (Zhou et al., 2016). See MGUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfb^f right) \n    tildemathbfh(t) = tanhleft( mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_hh left( mathbff(t) odot mathbfh(t-1) right) +\n        mathbfb^h right) \n    mathbfh(t) = left(1 - mathbff(t)right) odot mathbfh(t-1) +\n        mathbff(t) odot tildemathbfh(t)\nendaligned\n\nForward\n\nmgu(inp, state)\nmgu(inp)\n\nArguments\n\ninp: The input to the mgu. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MGU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/ran/#RAN","page":"RAN","title":"RAN","text":"","category":"section"},{"location":"api/layers/ran/#RecurrentLayers.RAN","page":"RAN","title":"RecurrentLayers.RAN","text":"RAN(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nRecurrent Additive Network cell (Lee et al., 2017). See RANCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    tildemathbfc(t) = mathbfW^c_ih mathbfx(t) +\n        mathbfb^c \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) + mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfb^f right) \n    mathbfc(t) = mathbfi(t) odot tildemathbfc(t) +\n        mathbff(t) odot mathbfc(t-1) \n    mathbfh(t) = gleft( mathbfc(t) right)\nendaligned\n\nForward\n\nran(inp, (state, cstate))\nran(inp)\n\nArguments\n\ninp: The input to the ran. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RAN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/sgrncell/#SGRNCell","page":"SGRNCell","title":"SGRNCell","text":"","category":"section"},{"location":"api/cells/sgrncell/#RecurrentLayers.SGRNCell","page":"SGRNCell","title":"RecurrentLayers.SGRNCell","text":"SGRNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform)\n\nSimple gated recurrent network (Zu and Wei, 2020). See SGRN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfW_hh mathbfh(t-1) + mathbfb right) \n    mathbfi(t) = 1 - mathbff(t) \n    mathbfh(t) = tanhleft( mathbfi(t) circ left(\n        mathbfW_ih mathbfx(t) right) + mathbff(t) circ\n        mathbfh(t-1) right)\nendaligned\n\nForward\n\nsgrncell(inp, state)\nsgrncell(inp)\n\nArguments\n\ninp: The input to the sgrncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the SGRNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/atr/#ATR","page":"ATR","title":"ATR","text":"","category":"section"},{"location":"api/layers/atr/#RecurrentLayers.ATR","page":"ATR","title":"RecurrentLayers.ATR","text":"ATR(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nAddition-subtraction twin-gated recurrent cell (Zhang et al., 2018). See ATRCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    mathbfp(t) = mathbfW_ih mathbfx(t) + mathbfb \n    mathbfq(t) = mathbfW_hh mathbfh(t-1) \n    mathbfi(t) = sigmaleft( mathbfp(t) + mathbfq(t) right) \n    mathbff(t) = sigmaleft( mathbfp(t) - mathbfq(t) right) \n    mathbfh(t) = mathbfi(t) circ mathbfp(t) + mathbff(t) circ\n        mathbfh(t-1)\nendaligned\n\n\nForward\n\natr(inp, state)\natr(inp)\n\nArguments\n\ninp: The input to the atr. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the ATR. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/rhn/#RHN","page":"RHN","title":"RHN","text":"","category":"section"},{"location":"api/layers/rhn/#RecurrentLayers.RHN","page":"RHN","title":"RecurrentLayers.RHN","text":"RHN(input_size => hidden_size, [depth];\n    return_state = false,\n    kwargs...)\n\nRecurrent highway network (Zilly et al., 06–11 Aug 2017). See RHNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndepth: depth of the recurrence. Default is 3\n\nKeyword arguments\n\ncouple_carry: couples the carry gate and the transform gate. Default true\ninit_kernel: initializer for the input to hidden weights\nbias: include a bias or not. Default is true\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfs_ell(t) = mathbfh_ell(t) odot mathbft_ell(t) +\n        mathbfs_ell-1(t) odot mathbfc_ell(t) \n    mathbfh_ell(t) = tanhleft( mathbfW^h_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^h_ell_hh mathbfs_ell-1(t)\n        + mathbfb^h_ell right) \n    mathbft_ell(t) = sigmaleft( mathbfW^t_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^t_ell_hh mathbfs_ell-1(t)\n        + mathbfb^t_ell right) \n    mathbfc_ell(t) = sigmaleft( mathbfW^c_ih mathbfx(t) \n        mathbbI_ell = 1 + mathbfW^c_ell_hh mathbfs_ell-1(t)\n        + mathbfb^c_ell right)\nendaligned\n\n\n\n\n\n","category":"type"},{"location":"api/cells/scrncell/#SCRNCell","page":"SCRNCell","title":"SCRNCell","text":"","category":"section"},{"location":"api/cells/scrncell/#RecurrentLayers.SCRNCell","page":"SCRNCell","title":"RecurrentLayers.SCRNCell","text":"SCRNCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, alpha = 0.0)\n\nStructurally contraint recurrent unit (Mikolov et al., 2014). See SCRN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nalpha: structural contraint. Default is 0.0.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = (1 - alpha)  mathbfW_ih^s mathbfx(t) +\n        alpha  mathbfs(t-1) \n    mathbfh(t) = sigmaleft( mathbfW_ih^h mathbfs(t) +\n        mathbfW_hh^h mathbfh(t-1) + mathbfb^h right) \n    mathbfy(t) = fleft( mathbfW_hh^y mathbfh(t) +\n        mathbfW_ih^y mathbfs(t) + mathbfb^y right)\nendaligned\n\nForward\n\nscrncell(inp, (state, cstate))\nscrncell(inp)\n\nArguments\n\ninp: The input to the scrncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the SCRNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/antisymmetricrnncell/#AntisymmetricRNNCell","page":"AntisymmetricRNNCell","title":"AntisymmetricRNNCell","text":"","category":"section"},{"location":"api/cells/antisymmetricrnncell/#RecurrentLayers.AntisymmetricRNNCell","page":"AntisymmetricRNNCell","title":"RecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, epsilon=1.0)\n\nAntisymmetric recurrent cell (Chang et al., 2019). See AntisymmetricRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nEquations\n\n    mathbfh(t) = mathbfh(t-1) + epsilon tanh left( mathbfW_ih\n        mathbfx(t) + left( mathbfW_hh - mathbfW_hh^top - gamma\n        mathbfI right) mathbfh(t-1) + mathbfb right)\n\nForward\n\nasymrnncell(inp, state)\nasymrnncell(inp)\n\nArguments\n\ninp: The input to the asymrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the AntisymmetricRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut2cell/#MUT2Cell","page":"MUT2Cell","title":"MUT2Cell","text":"","category":"section"},{"location":"api/cells/mut2cell/#RecurrentLayers.MUT2Cell","page":"MUT2Cell","title":"RecurrentLayers.MUT2Cell","text":"MUT2Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 2 cell (Jozefowicz et al., 07–09 Jul 2015). See MUT2 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t) + mathbfb^z right) \n    mathbfr(t) = sigmaleft( mathbfx(t) + mathbfW^r_hh\n        mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + mathbfW^h_ih\n        mathbfx(t) + mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/trnncell/#TRNNCell","page":"TRNNCell","title":"TRNNCell","text":"","category":"section"},{"location":"api/cells/trnncell/#RecurrentLayers.TRNNCell","page":"TRNNCell","title":"RecurrentLayers.TRNNCell","text":"TRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed recurrent unit (Balduzzi and Ghifary, 20–22 Jun 2016). See TRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW_ih mathbfx(t) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW_fh mathbfx(t) +\n        mathbfb^f right) \n    mathbfh(t) = mathbff(t) odot mathbfh(t-1) + left(1 -\n        mathbff(t)right) odot mathbfz(t)\nendaligned\n\nForward\n\ntrnncell(inp, state)\ntrnncell(inp)\n\nArguments\n\ninp: The input to the trnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/wrappers/stacked/#StackedRNN","page":"StackedRNN","title":"StackedRNN","text":"","category":"section"},{"location":"api/wrappers/stacked/#RecurrentLayers.StackedRNN","page":"StackedRNN","title":"RecurrentLayers.StackedRNN","text":"StackedRNN(rlayer, (input_size, hidden_size), args...;\n    num_layers = 1, dropout = 0.0, kwargs...)\n\nConstructs a stack of recurrent layers given the recurrent layer type.\n\nArguments:\n\nrlayer: Any recurrent layer such as MGU, RHN, etc... or Flux.RNN, Flux.LSTM, etc.\ninput_size: Defines the input dimension for the first layer.\nhidden_size: defines the dimension of the hidden layer.\nnum_layers: The number of layers to stack. Default is 1.\ndropout: Value of dropout to apply between recurrent layers. Default is 0.0.\nargs...: Additional positional arguments passed to the recurrent layer.\n\nKeyword arguments\n\nkwargs...: Additional keyword arguments passed to the recurrent layers.\n\nExamples\n\njulia> using RecurrentLayers\n\njulia> stac_rnn = StackedRNN(MGU, (3=>5); num_layers = 4)\nStackedRNN(\n  [\n    MGU(3 => 10),                       # 90 parameters\n    MGU(5 => 10),                       # 110 parameters\n    MGU(5 => 10),                       # 110 parameters\n    MGU(5 => 10),                       # 110 parameters\n  ],\n)         # Total: 12 trainable arrays, 420 parameters,\n          # plus 4 non-trainable, 20 parameters, summarysize 2.711 KiB.\n\n\n\n\n\n\n","category":"type"},{"location":"api/layers/fastgrnn/#FastGRNN","page":"FastGRNN","title":"FastGRNN","text":"","category":"section"},{"location":"api/layers/fastgrnn/#RecurrentLayers.FastGRNN","page":"FastGRNN","title":"RecurrentLayers.FastGRNN","text":"FastGRNN(input_size => hidden_size, [activation];\nreturn_state = false, kwargs...)\n\nFast recurrent neural network (Kusupati et al., 2018). See FastGRNNCell for a layer that processes a single sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_zeta: Initializer for the zeta parameter. Default is 1.0.\ninit_nu: Initializer for the nu parameter. Default is - 4.0.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    tildemathbfh(t) = tanhleft( mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_hh mathbfh(t-1) + mathbfb^h right) \n    mathbfh(t) = left( left( zeta (1 - mathbfz(t)) + nu right)\n        odot tildemathbfh(t) right) + mathbfz(t) odot mathbfh(t-1)\nendaligned\n\nForward\n\nfastgrnn(inp, state)\nfastgrnn(inp)\n\nArguments\n\ninp: The input to the fastgrnn. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastGRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/tgrucell/#TGRUCell","page":"TGRUCell","title":"TGRUCell","text":"","category":"section"},{"location":"api/cells/tgrucell/#RecurrentLayers.TGRUCell","page":"TGRUCell","title":"RecurrentLayers.TGRUCell","text":"TGRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStrongly typed gated recurrent unit (Balduzzi and Ghifary, 20–22 Jun 2016). See TGRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfx(t-1) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfx(t-1) + mathbfb^f right) \n    mathbfo(t) = tauleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfx(t-1) + mathbfb^o right) \n    mathbfh(t) = mathbff(t) odot mathbfh(t-1) +\n        mathbfz(t) odot mathbfo(t)\nendaligned\n\nForward\n\ntgrucell(inp, state)\ntgrucell(inp)\n\nArguments\n\ninp: The input to the tgrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the TGRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, inp) is the new hidden state together with the current input.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/unicornncell/#UnICORNNCell","page":"UnICORNNCell","title":"UnICORNNCell","text":"","category":"section"},{"location":"api/cells/unicornncell/#RecurrentLayers.UnICORNNCell","page":"UnICORNNCell","title":"RecurrentLayers.UnICORNNCell","text":"UnICORNNCell(input_size => hidden_size, [dt];\n    alpha=0.0, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nUndamped independent controlled oscillatory recurrent neural unit (Rusch and Mishra, 18–24 Jul 2021). See coRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\nalpha: Control parameter. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfz(t-1) - Delta t  hatsigma(mathbfc) odot left\n        sigmaleft( mathbfw odot mathbfh(t-1) +\n        mathbfW_ih mathbfx(t) + mathbfb right) +\n        alpha  mathbfh(t-1) right \n    mathbfh(t) = mathbfh(t-1) + Delta t  hatsigma(mathbfc) odot\n        mathbfz(t)\nendaligned\n\nForward\n\nunicornncell(inp, (state, cstate))\nunicornncell(inp)\n\nArguments\n\ninp: The input to the unicornncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the UnICORNNCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut1cell/#MUT1Cell","page":"MUT1Cell","title":"MUT1Cell","text":"","category":"section"},{"location":"api/cells/mut1cell/#RecurrentLayers.MUT1Cell","page":"MUT1Cell","title":"RecurrentLayers.MUT1Cell","text":"MUT1Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 1 cell (Jozefowicz et al., 07–09 Jul 2015). See MUT1 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfb^z right) \n    mathbfr(t) = sigmaleft( mathbfW^r_ih mathbfx(t) +\n        mathbfW^r_hh mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + tanhleft(\n        mathbfW^h_ih mathbfx(t) + mathbfb^h right) +\n        mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/rancell/#RANCell","page":"RANCell","title":"RANCell","text":"","category":"section"},{"location":"api/cells/rancell/#RecurrentLayers.RANCell","page":"RANCell","title":"RecurrentLayers.RANCell","text":"RANCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nRecurrent Additive Network cell (Lee et al., 2017). See RAN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    tildemathbfc(t) = mathbfW^c_ih mathbfx(t) +\n        mathbfb^c \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) + mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfb^f right) \n    mathbfc(t) = mathbfi(t) odot tildemathbfc(t) +\n        mathbff(t) odot mathbfc(t-1) \n    mathbfh(t) = gleft( mathbfc(t) right)\nendaligned\n\nForward\n\nrancell(inp, (state, cstate))\nrancell(inp)\n\nArguments\n\ninp: The input to the rancell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the RANCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut3cell/#MUT3Cell","page":"MUT3Cell","title":"MUT3Cell","text":"","category":"section"},{"location":"api/cells/mut3cell/#RecurrentLayers.MUT3Cell","page":"MUT3Cell","title":"RecurrentLayers.MUT3Cell","text":"MUT3Cell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nMutated unit 3 cell (Jozefowicz et al., 07–09 Jul 2015). See MUT3 for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh tanhleft( mathbfh(t) right) + mathbfb^z\n        right) \n    mathbfr(t) = sigmaleft( mathbfW^r_ih mathbfx(t) +\n        mathbfW^r_hh mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + mathbfW^h_ih\n        mathbfx(t) + mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmutcell(inp, state)\nmutcell(inp)\n\nArguments\n\ninp: The input to the mutcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the MUTCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/indrnn/#IndRNN","page":"IndRNN","title":"IndRNN","text":"","category":"section"},{"location":"api/layers/indrnn/#RecurrentLayers.IndRNN","page":"IndRNN","title":"RecurrentLayers.IndRNN","text":"IndRNN(input_size, hidden_size, [activation];\n    return_state = false, kwargs...)\n\nIndependently recurrent network (Li et al., Jun 2018). See IndRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\n    mathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) + mathbfu\n        odot mathbfh(t-1) + mathbfb right)\n\nForward\n\nindrnn(inp, state)\nindrnn(inp)\n\nArguments\n\ninp: The input to the indrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the IndRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/brcell/#BRCell","page":"BRCell","title":"BRCell","text":"","category":"section"},{"location":"api/cells/brcell/#RecurrentLayers.BRCell","page":"BRCell","title":"RecurrentLayers.BRCell","text":"BRCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform)\n\nBistable recurrent cell (Vecoven et al., 2021). See BR for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft( mathbfW^a_ih mathbfx(t) +\n        mathbfw^a circ mathbfh(t-1) + mathbfb^a right) \n    mathbfc(t) = sigmaleft( mathbfW^c_ih mathbfx(t) +\n        mathbfw^c circ mathbfh(t-1) + mathbfb^c right) \n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + left(1 - mathbfc(t)right)\n        circ tanhleft( mathbfW^h_ih mathbfx(t) + mathbfa(t) circ\n        mathbfh(t-1) + mathbfb^h right)\nendaligned\n\nForward\n\nbrcell(inp, state)\nbrcell(inp)\n\nArguments\n\ninp: The input to the brcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the BRCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/lightrucell/#LightRUCell","page":"LightRUCell","title":"LightRUCell","text":"","category":"section"},{"location":"api/cells/lightrucell/#RecurrentLayers.LightRUCell","page":"LightRUCell","title":"RecurrentLayers.LightRUCell","text":"LightRUCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nLight recurrent unit (Ye et al., 2024). See LightRU for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = tanhleft( mathbfW_ih^h mathbfx(t) right) \n    mathbff(t) = deltaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb^f right) \n    mathbfh(t) = left( 1 - mathbff(t) right) odot mathbfh(t-1) +\n        mathbff(t) odot tildemathbfh(t)\nendaligned\n\nForward\n\nlightrucell(inp, state)\nlightrucell(inp)\n\nArguments\n\ninp: The input to the lightrucell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the LightRUCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/mut2/#MUT2","page":"MUT2","title":"MUT2","text":"","category":"section"},{"location":"api/layers/mut2/#RecurrentLayers.MUT2","page":"MUT2","title":"RecurrentLayers.MUT2","text":"MUT2Cell(input_size => hidden_size;\n    return_state=false,\n    kwargs...)\n\nMutated unit 2 network (Jozefowicz et al., 07–09 Jul 2015). See MUT2Cell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t) + mathbfb^z right) \n    mathbfr(t) = sigmaleft( mathbfx(t) + mathbfW^r_hh\n        mathbfh(t) + mathbfb^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW^h_hh left(\n        mathbfr(t) odot mathbfh(t) right) + mathbfW^h_ih\n        mathbfx(t) + mathbfb^h right) right odot mathbfz(t) \n        quad + mathbfh(t) odot left( 1 - mathbfz(t) right)\nendaligned\n\nForward\n\nmut(inp, state)\nmut(inp)\n\nArguments\n\ninp: The input to the mut. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the MUT. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/nas/#NAS","page":"NAS","title":"NAS","text":"","category":"section"},{"location":"api/layers/nas/#RecurrentLayers.NAS","page":"NAS","title":"RecurrentLayers.NAS","text":"NAS(input_size => hidden_size;\n    return_state = false,\n    kwargs...)\n\nNeural Architecture Search unit (Zoph and Le, 2017). See NASCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfo_1 = sigmaleft( mathbfW^(1)_ih mathbfx(t) +\n        mathbfW^(1)_hh mathbfh(t-1) + mathbfb^(1) right) \n    mathbfo_2 = textReLUleft( mathbfW^(2)_ih mathbfx(t) +\n        mathbfW^(2)_hh mathbfh(t-1) + mathbfb^(2) right) \n    mathbfo_3 = sigmaleft( mathbfW^(3)_ih mathbfx(t) +\n        mathbfW^(3)_hh mathbfh(t-1) + mathbfb^(3) right) \n    mathbfo_4 = textReLUleft( mathbfW^(4)_ih mathbfx(t)\n        cdot mathbfW^(4)_hh mathbfh(t-1) right) \n    mathbfo_5 = tanhleft( mathbfW^(5)_ih mathbfx(t) +\n        mathbfW^(5)_hh mathbfh(t-1) + mathbfb^(5) right) \n    mathbfo_6 = sigmaleft( mathbfW^(6)_ih mathbfx(t) +\n        mathbfW^(6)_hh mathbfh(t-1) + mathbfb^(6) right) \n    mathbfo_7 = tanhleft( mathbfW^(7)_ih mathbfx(t) +\n        mathbfW^(7)_hh mathbfh(t-1) + mathbfb^(7) right) \n    mathbfo_8 = sigmaleft( mathbfW^(8)_ih mathbfx(t) +\n        mathbfW^(8)_hh mathbfh(t-1) + mathbfb^(8) right) 1ex\n    mathbfl_1 = tanhleft( mathbfo_1 cdot mathbfo_2 right) \n    mathbfl_2 = tanhleft( mathbfo_3 + mathbfo_4 right) \n    mathbfl_3 = tanhleft( mathbfo_5 cdot mathbfo_6 right) \n    mathbfl_4 = sigmaleft( mathbfo_7 + mathbfo_8 right) 1ex\n    mathbfl_1 = tanhleft( mathbfl_1 + mathbfc_textstate\n        right) 1ex\n    mathbfc_textnew = mathbfl_1 cdot mathbfl_2 \n    mathbfl_5 = tanhleft( mathbfl_3 + mathbfl_4 right) \n    mathbfh_textnew = tanhleft( mathbfc_textnew cdot\n        mathbfl_5 right)\nendaligned\n\nForward\n\nnas(inp, (state, cstate))\nnas(inp)\n\nArguments\n\ninp: The input to the nas. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the NAS.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/sgrn/#SGRN","page":"SGRN","title":"SGRN","text":"","category":"section"},{"location":"api/layers/sgrn/#RecurrentLayers.SGRN","page":"SGRN","title":"RecurrentLayers.SGRN","text":"SGRN(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nSimple gated recurrent network (Zu and Wei, 2020). See SGRNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfW_hh mathbfh(t-1) + mathbfb right) \n    mathbfi(t) = 1 - mathbff(t) \n    mathbfh(t) = tanhleft( mathbfi(t) circ left(\n        mathbfW_ih mathbfx(t) right) + mathbff(t) circ\n        mathbfh(t-1) right)\nendaligned\n\nForward\n\nsgrn(inp, state)\nsgrn(inp)\n\nArguments\n\ninp: The input to the sgrn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the SGRN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/peepholelstmcell/#PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"PeepholeLSTMCell","text":"","category":"section"},{"location":"api/cells/peepholelstmcell/#RecurrentLayers.PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"RecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    init_peephole_kernel = glorot_uniform,\n    bias = true)\n\nPeephole long short term memory cell (Gers et al., 2002). See PeepholeLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_peephole_kernel: initializer for the hidden to peephole weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) + mathbfw^i_ph odot\n        mathbfc(t-1) + mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) + mathbfw^f_ph odot\n        mathbfc(t-1) + mathbfb^f right) \n    mathbfc(t) = mathbff(t) odot mathbfc(t-1) + mathbfi(t)\n        odot mathbfz(t) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfh(t-1) + mathbfw^o_ph odot\n        mathbfc(t) + mathbfb^o right) \n    mathbfh(t) = mathbfo(t) odot tanhleft( mathbfc(t) right)\nendaligned\n\nForward\n\npeepholelstmcell(inp, (state, cstate))\npeepholelstmcell(inp)\n\nArguments\n\ninp: The input to the peepholelstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the PeepholeLSTMCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/unicornn/#UnICORNN","page":"UnICORNN","title":"UnICORNN","text":"","category":"section"},{"location":"api/layers/unicornn/#RecurrentLayers.UnICORNN","page":"UnICORNN","title":"RecurrentLayers.UnICORNN","text":"UnICORNN(input_size => hidden_size, [dt];\n    alpha=0.0, return_state=false, init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform, bias = true)\n\nUndamped independent controlled oscillatory recurrent neural network (Rusch and Mishra, 18–24 Jul 2021). See UnICORNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\nalpha: Control parameter. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfz(t-1) - Delta t  hatsigma(mathbfc) odot left\n        sigmaleft( mathbfw odot mathbfh(t-1) +\n        mathbfW_ih mathbfx(t) + mathbfb right) +\n        alpha  mathbfh(t-1) right \n    mathbfh(t) = mathbfh(t-1) + Delta t  hatsigma(mathbfc) odot\n        mathbfz(t)\nendaligned\n\nForward\n\nunicornn(inp, (state, zstate))\nunicornn(inp)\n\nArguments\n\ninp: The input to the unicornn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the UnICORNN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastgrnncell/#FastGRNNCell","page":"FastGRNNCell","title":"FastGRNNCell","text":"","category":"section"},{"location":"api/cells/fastgrnncell/#RecurrentLayers.FastGRNNCell","page":"FastGRNNCell","title":"RecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nFast gated recurrent neural network cell (Kusupati et al., 2018). See FastGRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_zeta: Initializer for the zeta parameter. Default is 1.0.\ninit_nu: Initializer for the nu parameter. Default is - 4.0.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    tildemathbfh(t) = tanhleft( mathbfW^h_ih mathbfx(t) +\n        mathbfW^h_hh mathbfh(t-1) + mathbfb^h right) \n    mathbfh(t) = left( left( zeta (1 - mathbfz(t)) + nu right)\n        odot tildemathbfh(t) right) + mathbfz(t) odot mathbfh(t-1)\nendaligned\n\nForward\n\nfastgrnncell(inp, state)\nfastgrnncell(inp)\n\nArguments\n\ninp: The input to the fastgrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the FastGRNN. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/ligru/#LiGRU","page":"LiGRU","title":"LiGRU","text":"","category":"section"},{"location":"api/layers/ligru/#RecurrentLayers.LiGRU","page":"LiGRU","title":"RecurrentLayers.LiGRU","text":"LiGRU(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nLight gated recurrent network (Ravanelli et al., 2018). The implementation does not include the batch normalization as described in the original paper. See LiGRUCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfh(t-1) + mathbfb^z right) \n    tildemathbfh(t) = textReLUleft( mathbfW^h_ih\n        mathbfx(t) + mathbfW^h_hh mathbfh(t-1) + mathbfb^h\n        right) \n    mathbfh(t) = mathbfz(t) odot mathbfh(t-1) + left(1 -\n        mathbfz(t)right) odot tildemathbfh(t)\nendaligned\n\nForward\n\nligru(inp, state)\nligru(inp)\n\nArguments\n\ninp: The input to the ligru. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the LiGRU. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/gatedantisymmetricrnncell/#GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"GatedAntisymmetricRNNCell","text":"","category":"section"},{"location":"api/cells/gatedantisymmetricrnncell/#RecurrentLayers.GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"RecurrentLayers.GatedAntisymmetricRNNCell","text":"GatedAntisymmetricRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, epsilon=1.0)\n\nAntisymmetric recurrent cell with gating (Chang et al., 2019). See GatedAntisymmetricRNN for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( left( mathbfW_hh - mathbfW_hh^top -\n        gamma mathbfI right) mathbfh(t-1) + mathbfW^z_ih\n        mathbfx(t) + mathbfb^z right) \n    mathbfh(t) = mathbfh(t-1) + epsilon  mathbfz(t) odot\n        tanhleft( left( mathbfW_hh - mathbfW_hh^top - gamma\n        mathbfI right) mathbfh(t-1) + mathbfW^h_ih mathbfx(t) +\n        mathbfb^h right)\nendaligned\n\nForward\n\nasymrnncell(inp, state)\nasymrnncell(inp)\n\nArguments\n\ninp: The input to the asymrnncell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the GatedAntisymmetricRNNCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/wmclstmcell/#WMCLSTMCell","page":"WMCLSTMCell","title":"WMCLSTMCell","text":"","category":"section"},{"location":"api/cells/wmclstmcell/#RecurrentLayers.WMCLSTMCell","page":"WMCLSTMCell","title":"RecurrentLayers.WMCLSTMCell","text":"WMCLSTMCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    init_memory_kernel = glorot_uniform,\n    bias = true)\n\nLong short term memory cell with working memory connections (Landi et al., 2021). See WMCLSTM for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_memory_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfi(t) = sigmaleft( mathbfW^i_ih mathbfx(t) +\n        mathbfW^i_hh mathbfh(t-1) +\n        tanhleft( mathbfW^i_ch mathbfc(t-1) right) +\n        mathbfb^i right) \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfh(t-1) +\n        tanhleft( mathbfW^f_ch mathbfc(t-1) right) +\n        mathbfb^f right) \n    mathbfo(t) = sigmaleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfh(t-1) +\n        tanhleft( mathbfW^o_ch mathbfc(t) right) +\n        mathbfb^o right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t) circ\n        sigma_cleft( mathbfW^c_ih mathbfx(t) + mathbfb^c right) \n    mathbfh(t) = mathbfo(t) circ sigma_hleft( mathbfc(t) right)\nendaligned\n\nForward\n\nwmclstmcell(inp, (state, cstate))\nwmclstmcell(inp)\n\nArguments\n\ninp: The input to the wmclstmcell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the WMCLSTMCell. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/fastrnn/#FastRNN","page":"FastRNN","title":"FastRNN","text":"","category":"section"},{"location":"api/layers/fastrnn/#RecurrentLayers.FastRNN","page":"FastRNN","title":"RecurrentLayers.FastRNN","text":"FastRNN(input_size => hidden_size, [activation];\n    return_state = false, kwargs...)\n\nFast recurrent neural network (Kusupati et al., 2018). See FastRNNCell for a layer that processes a single sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\ninit_alpha: Initializer for the alpha parameter. Default is 3.0.\ninit_beta: Initializer for the beta parameter. Default is - 3.0.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfW_hh mathbfh(t-1) + mathbfb right) \n    mathbfh(t) = alpha  tildemathbfh(t) + beta  mathbfh(t-1)\nendaligned\n\nForward\n\nfastrnn(inp, state)\nfastrnn(inp)\n\nArguments\n\ninp: The input to the fastrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the FastRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/trnn/#TRNN","page":"TRNN","title":"TRNN","text":"","category":"section"},{"location":"api/layers/trnn/#RecurrentLayers.TRNN","page":"TRNN","title":"RecurrentLayers.TRNN","text":"TRNN(input_size => hidden_size, [activation];\n    return_state = false, kwargs...)\n\nStrongly typed recurrent unit (Balduzzi and Ghifary, 20–22 Jun 2016). See TRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: activation function. Default is tanh.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW_ih mathbfx(t) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW_fh mathbfx(t) +\n        mathbfb^f right) \n    mathbfh(t) = mathbff(t) odot mathbfh(t-1) + left(1 -\n        mathbff(t)right) odot mathbfz(t)\nendaligned\n\nForward\n\ntrnn(inp, state)\ntrnn(inp)\n\nArguments\n\ninp: The input to the trnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/scrn/#SCRN","page":"SCRN","title":"SCRN","text":"","category":"section"},{"location":"api/layers/scrn/#RecurrentLayers.SCRN","page":"SCRN","title":"RecurrentLayers.SCRN","text":"SCRN(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true, alpha = 0.0,\n    return_state = false)\n\nStructurally contraint recurrent unit (Mikolov et al., 2014). See SCRNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nalpha: structural contraint. Default is 0.0.\nreturn_state: Option to return the last state together with the output. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = (1 - alpha)  mathbfW_ih^s mathbfx(t) +\n        alpha  mathbfs(t-1) \n    mathbfh(t) = sigmaleft( mathbfW_ih^h mathbfs(t) +\n        mathbfW_hh^h mathbfh(t-1) + mathbfb^h right) \n    mathbfy(t) = fleft( mathbfW_hh^y mathbfh(t) +\n        mathbfW_ih^y mathbfs(t) + mathbfb^y right)\nendaligned\n\nForward\n\nscrn(inp, (state, cstate))\nscrn(inp)\n\nArguments\n\ninp: The input to the scrn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\n(state, cstate): A tuple containing the hidden and cell states of the SCRN.  They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/fsrnn/#FSRNN","page":"FSRNN","title":"FSRNN","text":"","category":"section"},{"location":"api/layers/fsrnn/#RecurrentLayers.FSRNN","page":"FSRNN","title":"RecurrentLayers.FSRNN","text":"FSRNN(input_size => hidden_size,\n    fast_cells, slow_cell;\n    return_state=false)\n\nFast slow recurrent neural network (Mujika et al., 2017). See FSRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nfast_cells: a vector of the fast cells. Must be minimum of length 2.\nslow_cell: the chosen slow cell.\nreturn_state: option to return the last state. Default is false.\n\nEquations\n\nbeginaligned\n    mathbfh^F_1(t) = f^F_1left( mathbfh^F_k(t-1) mathbfx(t)\n        right) \n    mathbfh^S(t) = f^Sleft( mathbfh^S(t-1) mathbfh^F_1(t)\n        right) \n    mathbfh^F_2(t) = f^F_2left( mathbfh^F_1(t) mathbfh^S(t)\n        right) \n    mathbfh^F_i(t) = f^F_ileft( mathbfh^F_i-1(t) right) quad\n        textfor  3 leq i leq k\nendaligned\n\nForward\n\nfsrnn(inp, (fast_state, slow_state))\nfsrnn(inp)\n\nArguments\n\ninp: The input to the fsrnn. It should be a vector of size input_size or a matrix of size input_size x batch_size.\n(fast_state, slow_state): A tuple containing the hidden and cell states of the FSRNN. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/tlstm/#TLSTM","page":"TLSTM","title":"TLSTM","text":"","category":"section"},{"location":"api/layers/tlstm/#RecurrentLayers.TLSTM","page":"TLSTM","title":"RecurrentLayers.TLSTM","text":"TLSTM(input_size => hidden_size;\n    return_state = false, kwargs...)\n\nStrongly typed long short term memory (Balduzzi and Ghifary, 20–22 Jun 2016). See TLSTMCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\nreturn_state: Option to return the last state together with the output. Default is false.\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW^z_ih mathbfx(t) +\n        mathbfW^z_hh mathbfx(t-1) + mathbfb^z \n    mathbff(t) = sigmaleft( mathbfW^f_ih mathbfx(t) +\n        mathbfW^f_hh mathbfx(t-1) + mathbfb^f right) \n    mathbfo(t) = tauleft( mathbfW^o_ih mathbfx(t) +\n        mathbfW^o_hh mathbfx(t-1) + mathbfb^o right) \n    mathbfc(t) = mathbff(t) odot mathbfc(t-1) +\n        left(1 - mathbff(t)right) odot mathbfz(t) \n    mathbfh(t) = mathbfc(t) odot mathbfo(t)\nendaligned\n\nForward\n\ntlstm(inp, state)\ntlstm(inp)\n\nArguments\n\ninp: The input to the tlstm. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the TLSTM. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden states new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"This module provides several recurrent layers. In this page you can access their API documentation from this list:","category":"page"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"AntisymmetricRNNCell: Antisymmetric Recurrent Neural Network Cell\nATRCell: Addition-Subtraction Twin-Gated Recurrent Cell\nBRCell: Bistable Recurrent Cell\nCFNCell: Chaos Free Network Cell\ncoRNNCell: Coupled Oscillatory Recurrent Neural Network Cell\nFastGRNNCell: Fast  Gated Recurrent Neural Network Cell\nFastRNNCell: Fast Recurrent Neural Network Cell  \nFSRNNCell: FastSlow Recurrent Neural Network Cell\nGatedAntisymmetricRNNCell: Gated Antisymmetric Recurrent Neural Network Cell\nIndRNNCell: Independently Recurrent Neural Network Cell\nJANETCell: Just Another Network Cell\nLEMCell: Long Expressive Memory Cell\nLiGRUCell: Light Gated Recurrent Unit Cell\nLightRUCell: Light Recurrent Unit Cell\nMGUCell: Minimal Gated Unit Cell\nMinimalRNNCell: Minimal Recurrent Neural Network Cell\nMultiplicativeLSTMCell: Multiplicative Long Short-Term Memory Cell\nMUT1Cell: Mutated RNN Variant 1 Cell\nMUT2Cell: Mutated RNN Variant 2 Cell\nMUT3Cell: Mutated RNN Variant 3 Cell\nNASCell: Neural Architecture Search Cell\nNBRCell: Neuromodulated Bistable Recurrent Cell\nPeepholeLSTMCell: Peephole Long Short-Term Memory Cell\nRANCell: Recurrent Additive Network Cell\nRHNCell: Recurrent Highway Network Cell\nSCRNCell: Structurally Constrained Recurrent Network Cell\nSGRNCell: Simple Gated Recurrent Network Cell\nSTARCell: Stackable Recurrent Network Cell\nTGRUCell: Typed Gated Recurrent Unit Cell\nTLSTMCell: Typed Long Short-Term Memory Cell\nTRNNCell: Typed Recurrent Neural Network Cell\nUnICORNNCell: Undamped Independent Controlled Oscillatory Recurrent Neural Network Cell\nWMCLSTMCell: Working Memory Connection Long Short-Term Memory Cell","category":"page"},{"location":"api/layers/br/#BR","page":"BR","title":"BR","text":"","category":"section"},{"location":"api/layers/br/#RecurrentLayers.BR","page":"BR","title":"RecurrentLayers.BR","text":"BR(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nBistable recurrent network (Vecoven et al., 2021). See BRCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft( mathbfW^a_ih mathbfx(t) +\n        mathbfw^a circ mathbfh(t-1) + mathbfb^a right) \n    mathbfc(t) = sigmaleft( mathbfW^c_ih mathbfx(t) +\n        mathbfw^c circ mathbfh(t-1) + mathbfb^c right) \n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + left(1 - mathbfc(t)right)\n        circ tanhleft( mathbfW^h_ih mathbfx(t) + mathbfa(t) circ\n        mathbfh(t-1) + mathbfb^h right)\nendaligned\n\nForward\n\nbr(inp, state)\nbr(inp)\n\nArguments\n\ninp: The input to the br. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the BR. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"},{"location":"api/cells/starcell/#STARCell","page":"STARCell","title":"STARCell","text":"","category":"section"},{"location":"api/cells/starcell/#RecurrentLayers.STARCell","page":"STARCell","title":"RecurrentLayers.STARCell","text":"STARCell(input_size => hidden_size;\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nStackable recurrent cell (Turkoglu et al., 2021). See STAR for a layer that processes entire sequences.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW^z_ih mathbfx(t) +\n        mathbfb^z right) \n    mathbfk(t) = sigmaleft( mathbfW^k_ih mathbfx(t) +\n        mathbfW^k_hh mathbfh(t-1) + mathbfb^k right) \n    mathbfh(t) = tanhleft( left(1 - mathbfk(t)right) circ\n        mathbfh(t-1) + mathbfk(t) circ mathbfz(t) right)\nendaligned\n\nForward\n\nstarcell(inp, state)\nstarcell(inp)\n\nArguments\n\ninp: The input to the rancell. It should be a vector of size input_size or a matrix of size input_size x batch_size.\nstate: The hidden state of the STARCell. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size.\n\n\n\n\n\n","category":"type"},{"location":"api/wrappers/multiplicative/#Multiplicative","page":"Multiplicative","title":"Multiplicative","text":"","category":"section"},{"location":"api/wrappers/multiplicative/#RecurrentLayers.Multiplicative","page":"Multiplicative","title":"RecurrentLayers.Multiplicative","text":"Multiplicative(cell, inp, state)\n\nMultiplicative RNN (Sutskever et al., 2011). Wraps a given cell, and performs the following forward pass.\n\nCurrently this wrapper does not support the following cells:\n\nRHNCell\nRHNCellUnit\nFSRNNCell\nTLSTMCell\n\nbeginaligned\n    mathbfm_t   = (mathbfW_mx mathbfx_t) circ (mathbfW_mh mathbfh_t-1) \n    mathbfh_t = textcell(mathbfx_t mathbfm_t)\nendaligned\n\nArguments\n\nrcell: A recurrent cell constructor such as MGUCell, or\n\nFlux.LSTMCell etc.\n\ninput_size: Defines the input dimension for the first layer.\nhidden_size: defines the dimension of the hidden layer.\nargs...: positional arguments for the rcell.\n\nKeyword arguments\n\ninit_multiplicative_kernel:Initializer for the multiplicative input kernel. Default is glorot_uniform.\ninit_multiplicativerecurrent_kernel:Initializer for the multiplicative recurrent kernel. Default is glorot_uniform.\nkwargs...: keyword arguments for the rcell.\n\nForward\n\nmrnn(inp, state)\nmrnn(inp, (state, c_state))\nmrnn(inp)\n\nArguments\n\ninp: The input to the rcell. It should be a vector of size input_size\n\nor a matrix of size input_size x batch_size.\n\nstate: The hidden state of the rcell, is single return. It should be a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n(state, cstate): A tuple containing the hidden and cell states of the rcell. if double return. They should be vectors of size hidden_size or matrices of size hidden_size x batch_size. If not provided, they are assumed to be vectors of zeros, initialized by Flux.initialstates.\n\nReturns\n\nEither of \n\nA tuple (output, state), where both elements are given by the updated state new_state, a tensor of size hidden_size or hidden_size x batch_size, if the rcell is single return (e.g. Flux.RNNCell).\nA tuple (output, state), where output = new_state is the new hidden state and state = (new_state, new_cstate) is the new hidden and cell state.  They are tensors of size hidden_size or hidden_size x batch_size. This applies if the rcell is double return (e.g. Flux.LSTMCell).\n\nExamples\n\nWhen used to wrap a cell, Multiplicative will behave as the cell wrapped, taking input data in the same format, and returning states like the rcell would.\n\njulia> using RecurrentLayers\n\njulia> mrnn = Multiplicative(MGUCell, 3 => 5)\nMultiplicative(\n  5×3 Matrix{Float32},                  # 15 parameters\n  5×5 Matrix{Float32},                  # 25 parameters\n  MGUCell(3 => 5),                      # 90 parameters\n)                   # Total: 5 arrays, 130 parameters, 792 bytes.\n\n\nIn order to make Multiplicative act on a full sequence it is possible to wrap it in a Flux.Recurrence layer.\n\njulia> using RecurrentLayers, Flux\n\njulia> wrap = Recurrence(Multiplicative(AntisymmetricRNNCell, 2 => 4))\nRecurrence(\n  Multiplicative(\n    4×2 Matrix{Float32},                # 8 parameters\n    4×4 Matrix{Float32},                # 16 parameters\n    AntisymmetricRNNCell(2 => 4, tanh),  # 28 parameters\n  ),\n)                   # Total: 5 arrays, 52 parameters, 488 bytes.\n\n\n\n\n\n\n","category":"type"},{"location":"api/layers/gatedantisymmetricrnn/#GatedAntisymmetricRNN","page":"GatedAntisymmetricRNN","title":"GatedAntisymmetricRNN","text":"","category":"section"},{"location":"api/layers/gatedantisymmetricrnn/#RecurrentLayers.GatedAntisymmetricRNN","page":"GatedAntisymmetricRNN","title":"RecurrentLayers.GatedAntisymmetricRNN","text":"GatedAntisymmetricRNN(input_size, hidden_size;\n    return_state = false, kwargs...)\n\nAntisymmetric recurrent neural network with gating (Chang et al., 2019). See GatedAntisymmetricRNNCell for a layer that processes a single sequence.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( left( mathbfW_hh - mathbfW_hh^top -\n        gamma mathbfI right) mathbfh(t-1) + mathbfW^z_ih\n        mathbfx(t) + mathbfb^z right) \n    mathbfh(t) = mathbfh(t-1) + epsilon  mathbfz(t) odot\n        tanhleft( left( mathbfW_hh - mathbfW_hh^top - gamma\n        mathbfI right) mathbfh(t-1) + mathbfW^h_ih mathbfx(t) +\n        mathbfb^h right)\nendaligned\n\nForward\n\nasymrnn(inp, state)\nasymrnn(inp)\n\nArguments\n\ninp: The input to the asymrnn. It should be a vector of size input_size x len or a matrix of size input_size x len x batch_size.\nstate: The hidden state of the GatedAntisymmetricRNN. If given, it is a vector of size hidden_size or a matrix of size hidden_size x batch_size. If not provided, it is assumed to be a vector of zeros, initialized by Flux.initialstates.\n\nReturns\n\nNew hidden states new_states as an array of size hidden_size x len x batch_size. When return_state = true it returns a tuple of the hidden stats new_states and the last state of the iteration.\n\n\n\n\n\n","category":"type"}]
}
