var documenterSearchIndex = {"docs":
[{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"RANCell","category":"page"},{"location":"api/cells/#RecurrentLayers.RANCell","page":"Cells","title":"RecurrentLayers.RANCell","text":"RANCell(in => out; init = glorot_uniform, bias = true)\n\nThe RANCell, introduced in this paper,  is a recurrent cell layer which provides additional memory through the use of gates.\n\nand returns both ht anf ct.\n\nSee RAN for a layer that processes entire sequences.\n\nArguments\n\nin => out: Specifies the input and output dimensions of the layer.\ninit: Initialization function for the weight matrices, default is glorot_uniform.\nbias: Indicates if a bias term is included; the default is true.\n\nForward\n\nrancell(x, [h, c])\n\nThe forward pass takes the following arguments:\n\nx: Input to the cell, which can be a vector of size in or a matrix of size in x batch_size.\nh: The hidden state vector of the cell, sized out, or a matrix of size out x batch_size.\nc: The candidate state, sized out, or a matrix of size out x batch_size.\n\nIf not provided, both h and c default to vectors of zeros.\n\nExamples\n\nrancell = RANCell(3 => 5)\ninp = rand(Float32, 3)\n#initializing the hidden states, if we want to provide them\nstate = rand(Float32, 5)\nc_state = rand(Float32, 5)\n\n#result with default initialization of internal states\nresult = rancell(inp)\n#result with internal states provided\nresult_state = rancell(inp, (state, c_state))\n\n\n\n\n\n","category":"type"},{"location":"api/wrappers/#Cell-wrappers","page":"Cell Wrappers","title":"Cell wrappers","text":"","category":"section"},{"location":"api/wrappers/","page":"Cell Wrappers","title":"Cell Wrappers","text":"RAN","category":"page"},{"location":"api/wrappers/#RecurrentLayers.RAN","page":"Cell Wrappers","title":"RecurrentLayers.RAN","text":"RAN(in => out; init = glorot_uniform, bias = true)\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RecurrentLayers","category":"page"},{"location":"#RecurrentLayers","page":"Home","title":"RecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RecurrentLayers.jl extends Flux.jl recurrent layers offering by providing implementations of bleeding edge recurrent layers not commonly available in base deep learning libraries. It is designed for a seamless integration with the larger Flux ecosystem, enabling researchers and practitioners to leverage the latest developments in recurrent neural networks.","category":"page"}]
}
