<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Cells · RecurrentLayers.jl</title><meta name="title" content="Cells · RecurrentLayers.jl"/><meta property="og:title" content="Cells · RecurrentLayers.jl"/><meta property="twitter:title" content="Cells · RecurrentLayers.jl"/><meta name="description" content="Documentation for RecurrentLayers.jl."/><meta property="og:description" content="Documentation for RecurrentLayers.jl."/><meta property="twitter:description" content="Documentation for RecurrentLayers.jl."/><meta property="og:url" content="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/cells/"/><meta property="twitter:url" content="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/cells/"/><link rel="canonical" href="https://MartinuzziFrancesco.github.io/RecurrentLayers.jl/api/cells/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="RecurrentLayers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RecurrentLayers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Documentation</span><ul><li class="is-active"><a class="tocitem" href>Cells</a></li><li><a class="tocitem" href="../layers/">Layers</a></li><li><a class="tocitem" href="../wrappers/">Wrappers</a></li></ul></li><li><a class="tocitem" href="../../roadmap/">Roadmap</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Cells</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Cells</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/main/docs/src/api/cells.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Cells"><a class="docs-heading-anchor" href="#Cells">Cells</a><a id="Cells-1"></a><a class="docs-heading-anchor-permalink" href="#Cells" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.AntisymmetricRNNCell" href="#RecurrentLayers.AntisymmetricRNNCell"><code>RecurrentLayers.AntisymmetricRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AntisymmetricRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, epsilon=1.0)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent cell</a>. See <a href="../layers/#RecurrentLayers.AntisymmetricRNN"><code>AntisymmetricRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: activation function. Default is <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[h_t = h_{t-1} + \epsilon \tanh ( (W_h - W_h^T - \gamma I) h_{t-1} + V_h x_t + b_h ),\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">asymrnncell(inp, state)
asymrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the asymrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the AntisymmetricRNNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/antisymmetricrnn_cell.jl#L2-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.ATRCell" href="#RecurrentLayers.ATRCell"><code>RecurrentLayers.ATRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ATRCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform)</code></pre><p><a href="https://arxiv.org/abs/1810.12546">Addition-subtraction twin-gated recurrent cell</a>. See <a href="../layers/#RecurrentLayers.ATR"><code>ATR</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{p}_t &amp;= \mathbf{W} \mathbf{x}_t, \\
    \mathbf{q}_t &amp;= \mathbf{U} \mathbf{h}_{t-1}, \\
    \mathbf{i}_t &amp;= \sigma(\mathbf{p}_t + \mathbf{q}_t), \\
    \mathbf{f}_t &amp;= \sigma(\mathbf{p}_t - \mathbf{q}_t), \\
    \mathbf{h}_t &amp;= \mathbf{i}_t \circ \mathbf{p}_t + \mathbf{f}_t \circ \mathbf{h}_{t-1}.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">atrcell(inp, state)
atrcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the atrcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the ATRCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/atr_cell.jl#L2-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.BRCell" href="#RecurrentLayers.BRCell"><code>RecurrentLayers.BRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BRCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform)</code></pre><p><a href="https://doi.org/10.1371/journal.pone.0252676">Bistable recurrent cell</a>. See <a href="../layers/#RecurrentLayers.BR"><code>BR</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{h}_t &amp;= \mathbf{c}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{c}_t)
        \circ \tanh\left(\mathbf{U}_x \mathbf{x}_t + \mathbf{a}_t \circ
        \mathbf{h}_{t-1}\right), \\
    \mathbf{a}_t &amp;= 1 + \tanh\left(\mathbf{U}_a \mathbf{x}_t +
        \mathbf{w}_a \circ \mathbf{h}_{t-1}\right), \\
    \mathbf{c}_t &amp;= \sigma\left(\mathbf{U}_c \mathbf{x}_t + \mathbf{w}_c \circ
        \mathbf{h}_{t-1}\right).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">brcell(inp, state)
brcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the brcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the BRCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/br_cell.jl#L2-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.CFNCell" href="#RecurrentLayers.CFNCell"><code>RecurrentLayers.CFNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CFNCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1612.06212">Chaos free network unit</a>. See <a href="../layers/#RecurrentLayers.CFN"><code>CFN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    h_t &amp;= \theta_t \odot \tanh(h_{t-1}) + \eta_t \odot \tanh(W x_t), \\
    \theta_t &amp;:= \sigma (U_\theta h_{t-1} + V_\theta x_t + b_\theta), \\
    \eta_t &amp;:= \sigma (U_\eta h_{t-1} + V_\eta x_t + b_\eta).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">cfncell(inp, state)
cfncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the cfncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the CFNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/cfn_cell.jl#L2-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.coRNNCell" href="#RecurrentLayers.coRNNCell"><code>RecurrentLayers.coRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coRNNCell(input_size =&gt; hidden_size, [dt];
    gamma=0.0, epsilon=0.0,
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/2010.00951">Coupled oscillatory recurrent neural unit</a>. See <a href="../layers/#RecurrentLayers.coRNN"><code>coRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>dt</code>: time step. Default is 1.0.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>gamma</code>: damping for state. Default is 0.0.</li><li><code>epsilon</code>: damping for candidate state. Default is 0.0.</li><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\mathbf{y}_n &amp;= y_{n-1} + \Delta t \mathbf{z}_n, \\
\mathbf{z}_n &amp;= z_{n-1} + \Delta t \sigma \left( \mathbf{W} y_{n-1} +
    \mathcal{W} z_{n-1} + \mathbf{V} u_n + \mathbf{b} \right) -
    \Delta t \gamma y_{n-1} - \Delta t \epsilon \mathbf{z}_n,
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">cornncell(inp, (state, cstate))
cornncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the cornncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the coRNNCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/cornn_cell.jl#L2-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FastGRNNCell" href="#RecurrentLayers.FastGRNNCell"><code>RecurrentLayers.FastGRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastGRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast gated recurrent neural network cell</a>. See <a href="../layers/#RecurrentLayers.FastGRNN"><code>FastGRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_zeta</code>: Initializer for the zeta parameter. Default is 1.0.</li><li><code>init_nu</code>: Initializer for the nu parameter. Default is - 4.0.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z_t &amp;= \sigma(W x_t + U h_{t-1} + b_z), \\
\tilde{h}_t &amp;= \tanh(W x_t + U h_{t-1} + b_h), \\
h_t &amp;= \big((\zeta (1 - z_t) + \nu) \odot \tilde{h}_t\big) + z_t \odot h_{t-1}
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fastgrnncell(inp, state)
fastgrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fastgrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the FastGRNN. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/fastrnn_cell.jl#L171-L223">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FastRNNCell" href="#RecurrentLayers.FastRNNCell"><code>RecurrentLayers.FastRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    init_alpha = 3.0, init_beta = - 3.0,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast recurrent neural network cell</a>. See <a href="../layers/#RecurrentLayers.FastRNN"><code>FastRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_alpha</code>: Initializer for the alpha parameter. Default is 3.0.</li><li><code>init_beta</code>: Initializer for the beta parameter. Default is - 3.0.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{h}_t &amp;= \sigma(W_h x_t + U_h h_{t-1} + b), \\
h_t &amp;= \alpha \tilde{h}_t + \beta h_{t-1}
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fastrnncell(inp, state)
fastrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fastrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the FastRNN. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/fastrnn_cell.jl#L2-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.FSRNNCell" href="#RecurrentLayers.FSRNNCell"><code>RecurrentLayers.FSRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FSRNNCell(input_size =&gt; hidden_size,
    fast_cells, slow_cell)</code></pre><p><a href="https://arxiv.org/abs/1705.08639">Fast slow recurrent neural network cell</a>. See <a href="../layers/#RecurrentLayers.FSRNN"><code>FSRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>fast_cells</code>: a vector of the fast cells. Must be minimum of length 2.</li><li><code>slow_cell</code>: the chosen slow cell.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    h_t^{F_1} &amp;= f^{F_1}\left(h_{t-1}^{F_k}, x_t\right) \\
    h_t^S &amp;= f^S\left(h_{t-1}^S, h_t^{F_1}\right) \\
    h_t^{F_2} &amp;= f^{F_2}\left(h_t^{F_1}, h_t^S\right) \\
    h_t^{F_i} &amp;= f^{F_i}\left(h_t^{F_{i-1}}\right) \quad \text{for } 3 \leq i \leq k
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">fsrnncell(inp, (fast_state, slow_state))
fsrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the fsrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(fast_state, slow_state)</code>: A tuple containing the hidden and cell states of the FSRNNCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (fast_state, slow_state)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/fsrnn_cell.jl#L2-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.GatedAntisymmetricRNNCell" href="#RecurrentLayers.GatedAntisymmetricRNNCell"><code>RecurrentLayers.GatedAntisymmetricRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GatedAntisymmetricRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, epsilon=1.0)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent cell with gating</a>. See <a href="../layers/#RecurrentLayers.GatedAntisymmetricRNN"><code>GatedAntisymmetricRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \sigma ( (W_h - W_h^T - \gamma I) h_{t-1} + V_z x_t + b_z ), \\
    h_t &amp;= h_{t-1} + \epsilon z_t \odot \tanh ( (W_h - W_h^T - \gamma I) h_{t-1}
        + V_h x_t + b_h ).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">asymrnncell(inp, state)
asymrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the asymrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the GatedAntisymmetricRNNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/antisymmetricrnn_cell.jl#L158-L207">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.IndRNNCell" href="#RecurrentLayers.IndRNNCell"><code>RecurrentLayers.IndRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/1803.04831">Independently recurrent cell</a>. See <a href="../layers/#RecurrentLayers.IndRNN"><code>IndRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: activation function. Default is <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\mathbf{h}_{t} = \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{u} \odot \mathbf{h}_{t-1} +
    \mathbf{b})\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">indrnncell(inp, state)
indrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the indrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the IndRNNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/indrnn_cell.jl#L3-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.JANETCell" href="#RecurrentLayers.JANETCell"><code>RecurrentLayers.JANETCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">JANETCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, beta_value=1.0)</code></pre><p><a href="https://arxiv.org/abs/1804.04849">Just another network unit</a>. See <a href="../layers/#RecurrentLayers.JANET"><code>JANET</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li><li><code>beta_value</code>: control over the input data flow. Default is 1.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{s}_t &amp;= \mathbf{U}_f \mathbf{h}_{t-1} + \mathbf{W}_f \mathbf{x}_t +
        \mathbf{b}_f \\
    \tilde{\mathbf{c}}_t &amp;= \tanh (\mathbf{U}_c \mathbf{h}_{t-1} + \mathbf{W}_c
        \mathbf{x}_t + \mathbf{b}_c) \\
    \mathbf{c}_t &amp;= \sigma(\mathbf{s}_t) \odot \mathbf{c}_{t-1} + (1 - \sigma
        (\mathbf{s}_t - \beta)) \odot \tilde{\mathbf{c}}_t \\
    \mathbf{h}_t &amp;= \mathbf{c}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">janetcell(inp, (state, cstate))
janetcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the rancell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the RANCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/janet_cell.jl#L2-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LEMCell" href="#RecurrentLayers.LEMCell"><code>RecurrentLayers.LEMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LEMCell(input_size =&gt; hidden_size, [dt];
    init_kernel = glorot_uniform, init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/2110.04744">Long expressive memory unit</a>. See <a href="../layers/#RecurrentLayers.LEM"><code>LEM</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>dt</code>: timestep. Defaul is 1.0.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\boldsymbol{\Delta t_n} &amp;= \Delta \hat{t} \hat{\sigma}
    (W_1 y_{n-1} + V_1 u_n + b_1) \\
\overline{\boldsymbol{\Delta t_n}} &amp;= \Delta \hat{t}
    \hat{\sigma} (W_2 y_{n-1} + V_2 u_n + b_2) \\
z_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot z_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_z y_{n-1} + V_z u_n + b_z) \\
y_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot y_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_y z_n + V_y u_n + b_y)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">lemcell(inp, (state, cstate))
lemcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the lemcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the RANCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/lem_cell.jl#L2-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LiGRUCell" href="#RecurrentLayers.LiGRUCell"><code>RecurrentLayers.LiGRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LiGRUCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/1803.10225">Light gated recurrent unit</a>. The implementation does not include the batch normalization as described in the original paper. See <a href="../layers/#RecurrentLayers.LiGRU"><code>LiGRU</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z_t &amp;= \sigma(W_z x_t + U_z h_{t-1}), \\
\tilde{h}_t &amp;= \text{ReLU}(W_h x_t + U_h h_{t-1}), \\
h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">ligrucell(inp, state)
ligrucell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the ligrucell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the LiGRUCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/ligru_cell.jl#L2-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.LightRUCell" href="#RecurrentLayers.LightRUCell"><code>RecurrentLayers.LightRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LightRUCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://www.mdpi.com/2079-9292/13/16/3204">Light recurrent unit</a>. See <a href="../layers/#RecurrentLayers.LightRU"><code>LightRU</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{h}_t &amp;= \tanh(W_h x_t), \\
f_t         &amp;= \delta(W_f x_t + U_f h_{t-1} + b_f), \\
h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">lightrucell(inp, state)
lightrucell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the lightrucell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the LightRUCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/lightru_cell.jl#L3-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MGUCell" href="#RecurrentLayers.MGUCell"><code>RecurrentLayers.MGUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MGUCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/1603.09420">Minimal gated unit</a>. See <a href="../layers/#RecurrentLayers.MGU"><code>MGU</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
\tilde{h}_t &amp;= \tanh(W_h x_t + U_h (f_t \odot h_{t-1}) + b_h), \\
h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mgucell(inp, state)
mgucell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mgucell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the MGUCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/mgu_cell.jl#L2-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MinimalRNNCell" href="#RecurrentLayers.MinimalRNNCell"><code>RecurrentLayers.MinimalRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MinimalRNNCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, encoder_bias = true)</code></pre><p><a href="https://arxiv.org/abs/1711.06788">Minimal recurrent neural network unit</a>. See <a href="../layers/#RecurrentLayers.MinimalRNN"><code>MinimalRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li><li><code>encoder_bias</code>: include a bias in the encoder or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{z}_t &amp;= \Phi(\mathbf{x}_t) = \tanh(\mathbf{W}_x \mathbf{x}_t +
        \mathbf{b}_z), \\
    \mathbf{u}_t &amp;= \sigma(\mathbf{U}_h \mathbf{h}_{t-1} + \mathbf{U}_z \mathbf{z}_t +
        \mathbf{b}_u), \\
    \mathbf{h}_t &amp;= \mathbf{u}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{u}_t) \circ
        \mathbf{z}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">minimalrnncell(inp, state)
minimalrnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the minimalrnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the MinimalRNNCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/minimalrnn_cell.jl#L2-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MultiplicativeLSTMCell" href="#RecurrentLayers.MultiplicativeLSTMCell"><code>RecurrentLayers.MultiplicativeLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiplicativeLSTMCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    init_multiplicative_kernel=glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1609.07959">Multiplicative long short term memory cell</a>. See <a href="../layers/#RecurrentLayers.MultiplicativeLSTM"><code>MultiplicativeLSTM</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_multiplicative_kernel</code>: initializer for the multiplicative to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{m}_t &amp;= (\mathbf{W}_{mx} \mathbf{x}_t) \circ (\mathbf{W}_{mh}
        \mathbf{h}_{t-1}), \\
    \hat{\mathbf{h}}_t &amp;= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hm}
        \mathbf{m}_t, \\
    \mathbf{i}_t &amp;= \sigma(\mathbf{W}_{ix} \mathbf{x}_t + \mathbf{W}_{im}
        \mathbf{m}_t), \\
    \mathbf{o}_t &amp;= \sigma(\mathbf{W}_{ox} \mathbf{x}_t + \mathbf{W}_{om}
        \mathbf{m}_t), \\
    \mathbf{f}_t &amp;= \sigma(\mathbf{W}_{fx} \mathbf{x}_t + \mathbf{W}_{fm}
        \mathbf{m}_t), \\
    \mathbf{c}_t &amp;= \mathbf{f}_t \circ \mathbf{c}_{t-1} + \mathbf{i}_t \circ
        \tanh(\hat{\mathbf{h}}_t), \\
    \mathbf{h}_t &amp;= \tanh(\mathbf{c}_t) \circ \mathbf{o}_t.
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">multiplicativelstmcell(inp, (state, cstate))
multiplicativelstmcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the multiplicativelstmcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the MultiplicativeLSTMCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/multiplicativelstm_cell.jl#L2-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT1Cell" href="#RecurrentLayers.MUT1Cell"><code>RecurrentLayers.MUT1Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT1Cell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 1 cell</a>. See <a href="../layers/#RecurrentLayers.MUT1"><code>MUT1</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + b_z), \\
r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + \tanh(W_h x_t) + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mutcell(inp, state)
mutcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mutcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUTCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/mut_cell.jl#L2-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT2Cell" href="#RecurrentLayers.MUT2Cell"><code>RecurrentLayers.MUT2Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT2Cell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 cell</a>. See <a href="../layers/#RecurrentLayers.MUT2"><code>MUT2</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
r &amp;= \sigma(x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mutcell(inp, state)
mutcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mutcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUTCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/mut_cell.jl#L161-L208">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.MUT3Cell" href="#RecurrentLayers.MUT3Cell"><code>RecurrentLayers.MUT3Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT3Cell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 3 cell</a>. See <a href="../layers/#RecurrentLayers.MUT3"><code>MUT3</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z &amp;= \sigma(W_z x_t + U_z \tanh(h_t) + b_z), \\
r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
&amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">mutcell(inp, state)
mutcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the mutcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the MUTCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/mut_cell.jl#L319-L366">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.NASCell" href="#RecurrentLayers.NASCell"><code>RecurrentLayers.NASCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NASCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/1611.01578">Neural Architecture Search unit</a>. See <a href="../layers/#RecurrentLayers.NAS"><code>NAS</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\text{First Layer Outputs:} &amp; \\
o_1 &amp;= \sigma(W_i^{(1)} x_t + W_h^{(1)} h_{t-1} + b^{(1)}), \\
o_2 &amp;= \text{ReLU}(W_i^{(2)} x_t + W_h^{(2)} h_{t-1} + b^{(2)}), \\
o_3 &amp;= \sigma(W_i^{(3)} x_t + W_h^{(3)} h_{t-1} + b^{(3)}), \\
o_4 &amp;= \text{ReLU}(W_i^{(4)} x_t \cdot W_h^{(4)} h_{t-1}), \\
o_5 &amp;= \tanh(W_i^{(5)} x_t + W_h^{(5)} h_{t-1} + b^{(5)}), \\
o_6 &amp;= \sigma(W_i^{(6)} x_t + W_h^{(6)} h_{t-1} + b^{(6)}), \\
o_7 &amp;= \tanh(W_i^{(7)} x_t + W_h^{(7)} h_{t-1} + b^{(7)}), \\
o_8 &amp;= \sigma(W_i^{(8)} x_t + W_h^{(8)} h_{t-1} + b^{(8)}). \\

\text{Second Layer Computations:} &amp; \\
l_1 &amp;= \tanh(o_1 \cdot o_2) \\
l_2 &amp;= \tanh(o_3 + o_4) \\
l_3 &amp;= \tanh(o_5 \cdot o_6) \\
l_4 &amp;= \sigma(o_7 + o_8) \\

\text{Inject Cell State:} &amp; \\
l_1 &amp;= \tanh(l_1 + c_{\text{state}}) \\

\text{Final Layer Computations:} &amp; \\
c_{\text{new}} &amp;= l_1 \cdot l_2 \\
l_5 &amp;= \tanh(l_3 + l_4) \\
h_{\text{new}} &amp;= \tanh(c_{\text{new}} \cdot l_5)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">nascell(inp, (state, cstate))
nascell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the nascell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the NASCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/nas_cell.jl#L26-L94">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.NBRCell" href="#RecurrentLayers.NBRCell"><code>RecurrentLayers.NBRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NBRCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform)</code></pre><p><a href="https://doi.org/10.1371/journal.pone.0252676">Recurrently neuromodulated bistable recurrent cell</a>. See <a href="../layers/#RecurrentLayers.NBR"><code>NBR</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{h}_t &amp;= \mathbf{c}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{c}_t)
        \circ \tanh\left(\mathbf{U}_x \mathbf{x}_t + \mathbf{a}_t \circ
        \mathbf{h}_{t-1}\right), \\
    \mathbf{a}_t &amp;= 1 + \tanh\left(\mathbf{U}_a \mathbf{x}_t + \mathbf{W}_a
        \mathbf{h}_{t-1}\right), \\
    \mathbf{c}_t &amp;= \sigma\left(\mathbf{U}_c \mathbf{x}_t + \mathbf{W}_c
        \mathbf{h}_{t-1}\right), \\
    \mathbf{h}_t &amp;= \mathbf{c}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{c}_t)
        \circ \tanh\left(\mathbf{U}_x \mathbf{x}_t + \mathbf{a}_t \circ
        \mathbf{h}_{t-1}\right).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">nbrcell(inp, state)
nbrcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the nbrcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the NBRCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/br_cell.jl#L170-L224">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.PeepholeLSTMCell" href="#RecurrentLayers.PeepholeLSTMCell"><code>RecurrentLayers.PeepholeLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PeepholeLSTMCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    init_peephole_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Peephole long short term memory cell</a>. See <a href="../layers/#RecurrentLayers.PeepholeLSTM"><code>PeepholeLSTM</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_peephole_kernel</code>: initializer for the hidden to peephole weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \tanh(W_z x_t + U_z h_{t-1} + b_z), \\
    i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + p_i \odot c_{t-1} + b_i), \\
    f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + p_f \odot c_{t-1} + b_f), \\
    c_t &amp;= f_t \odot c_{t-1} + i_t \odot z_t, \\
    o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + p_o \odot c_t + b_o), \\
    h_t &amp;= o_t \odot \tanh(c_t).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">peepholelstmcell(inp, (state, cstate))
peepholelstmcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the peepholelstmcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the PeepholeLSTMCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/peepholelstm_cell.jl#L2-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.RANCell" href="#RecurrentLayers.RANCell"><code>RecurrentLayers.RANCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RANCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/pdf/1705.07393">Recurrent Additive Network cell</a>. See <a href="../layers/#RecurrentLayers.RAN"><code>RAN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{c}_t &amp;= W_c x_t, \\
i_t         &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i), \\
f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
c_t         &amp;= i_t \odot \tilde{c}_t + f_t \odot c_{t-1}, \\
h_t         &amp;= g(c_t)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">rancell(inp, (state, cstate))
rancell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the rancell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the RANCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/ran_cell.jl#L2-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.RHNCell" href="#RecurrentLayers.RHNCell"><code>RecurrentLayers.RHNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RHNCell(input_size =&gt; hidden_size, [depth];
    couple_carry = true,
    cell_kwargs...)</code></pre><p><a href="https://arxiv.org/pdf/1607.03474">Recurrent highway network</a>. See <a href="#RecurrentLayers.RHNCellUnit"><code>RHNCellUnit</code></a> for a the unit component of this layer. See <a href="../layers/#RecurrentLayers.RHN"><code>RHN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>depth</code>: depth of the recurrence. Default is 3.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>couple_carry</code>: couples the carry gate and the transform gate. Default <code>true</code></li><li><code>init_kernel</code>: initializer for the input to hidden weights. Default is <code>glorot_uniform</code></li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_{\ell}^{[t]} &amp;= h_{\ell}^{[t]} \odot t_{\ell}^{[t]} + s_{\ell-1}^{[t]}
    \odot c_{\ell}^{[t]}, \\
\text{where} \\
h_{\ell}^{[t]} &amp;= \tanh(W_h x^{[t]}\mathbb{I}_{\ell = 1} + U_{h_{\ell}}
    s_{\ell-1}^{[t]} + b_{h_{\ell}}), \\
t_{\ell}^{[t]} &amp;= \sigma(W_t x^{[t]}\mathbb{I}_{\ell = 1} + U_{t_{\ell}}
    s_{\ell-1}^{[t]} + b_{t_{\ell}}), \\
c_{\ell}^{[t]} &amp;= \sigma(W_c x^{[t]}\mathbb{I}_{\ell = 1} + U_{c_{\ell}}
    s_{\ell-1}^{[t]} + b_{c_{\ell}})
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">rnncell(inp, [state])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/rhn_cell.jl#L46-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.RHNCellUnit" href="#RecurrentLayers.RHNCellUnit"><code>RecurrentLayers.RHNCellUnit</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RHNCellUnit(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    bias = true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/rhn_cell.jl#L4-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.SCRNCell" href="#RecurrentLayers.SCRNCell"><code>RecurrentLayers.SCRNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SCRNCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true, alpha = 0.0)</code></pre><p><a href="https://arxiv.org/pdf/1412.7753">Structurally contraint recurrent unit</a>. See <a href="../layers/#RecurrentLayers.SCRN"><code>SCRN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li><li><code>alpha</code>: structural contraint. Default is 0.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_t &amp;= (1 - \alpha) W_s x_t + \alpha s_{t-1}, \\
h_t &amp;= \sigma(W_h s_t + U_h h_{t-1} + b_h), \\
y_t &amp;= f(U_y h_t + W_y s_t)
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">scrncell(inp, (state, cstate))
scrncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the scrncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the SCRNCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/scrn_cell.jl#L3-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.SGRNCell" href="#RecurrentLayers.SGRNCell"><code>RecurrentLayers.SGRNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SGRNCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform)</code></pre><p><a href="https://doi.org/10.1049/gtd2.12056">Simple gated recurrent network</a>. See <a href="../layers/#RecurrentLayers.SGRN"><code>SGRN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
        \mathbf{f}_t &amp;= \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{U} \mathbf{h}_{t-1} +
            \mathbf{b}), \\
    \mathbf{i}_t &amp;= 1 - \mathbf{f}_t, \\
    \mathbf{h}_t &amp;= \tanh\left(\mathbf{i}_t \circ (\mathbf{W} \mathbf{x}_t) +
        \mathbf{f}_t \circ \mathbf{h}_{t-1}\right).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">sgrncell(inp, state)
sgrncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the sgrncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the SGRNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/sgrn_cell.jl#L2-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.STARCell" href="#RecurrentLayers.STARCell"><code>RecurrentLayers.STARCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">STARCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1911.11033">Stackable recurrent cell</a>. See <a href="../layers/#RecurrentLayers.STAR"><code>STAR</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \tanh(W_z x_t + b_z), \\
    k_t &amp;= \sigma(W_x x_t + W_h h_{t-1} + b_k), \\
    h_t &amp;= \tanh\left((1 - k_t) \circ h_{t-1} + k_t \circ z_t\right).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">starcell(inp, state)
starcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the rancell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the STARCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/star_cell.jl#L2-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TGRUCell" href="#RecurrentLayers.TGRUCell"><code>RecurrentLayers.TGRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TGRUCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed gated recurrent unit</a>. See <a href="../layers/#RecurrentLayers.TGRU"><code>TGRU</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t +
        \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    h_t &amp;= f_t \odot h_{t-1} + z_t \odot o_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">tgrucell(inp, state)
tgrucell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the tgrucell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the TGRUCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, inp)</code> is the new hidden state together with the current input.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/trnn_cell.jl#L157-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TLSTMCell" href="#RecurrentLayers.TLSTMCell"><code>RecurrentLayers.TLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TLSTMCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed long short term memory cell</a>. See <a href="../layers/#RecurrentLayers.TLSTM"><code>TLSTM</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t +
        \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    c_t &amp;= f_t \odot c_{t-1} + (1 - f_t) \odot z_t \\
    h_t &amp;= c_t \odot o_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">tlstmcell(inp, state)
tlstmcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the tlstmcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the TLSTMCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate, inp)</code> is the new hidden and cell state, together with the current input.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/trnn_cell.jl#L322-L373">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.TRNNCell" href="#RecurrentLayers.TRNNCell"><code>RecurrentLayers.TRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed recurrent unit</a>. See <a href="../layers/#RecurrentLayers.TRNN"><code>TRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: activation function. Default is <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{W} x_t \\
    f_t &amp;= \sigma (\mathbf{V} x_t + b) \\
    h_t &amp;= f_t \odot h_{t-1} + (1 - f_t) \odot z_t
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">trnncell(inp, state)
trnncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the trnncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>state</code>: The hidden state of the TRNNCell. It should be a vector of size <code>hidden_size</code> or a matrix of size <code>hidden_size x batch_size</code>. If not provided, it is assumed to be a vector of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where both elements are given by the updated state <code>new_state</code>, a tensor of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/trnn_cell.jl#L2-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.UnICORNNCell" href="#RecurrentLayers.UnICORNNCell"><code>RecurrentLayers.UnICORNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnICORNNCell(input_size =&gt; hidden_size, [dt];
    alpha=0.0, init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform, bias = true)</code></pre><p><a href="https://arxiv.org/abs/2103.05487">Undamped independent controlled oscillatory recurrent neural unit</a>. See <a href="../layers/#RecurrentLayers.coRNN"><code>coRNN</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>dt</code>: time step. Default is 1.0.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>alpha</code>: Control parameter. Default is 0.0.</li><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    y_n &amp;= y_{n-1} + \Delta t \, \hat{\sigma}(c) \odot z_n, \\
    z_n &amp;= z_{n-1} - \Delta t \, \hat{\sigma}(c) \odot \left[ 
        \sigma \left( w \odot y_{n-1} + V y_{n-1} + b \right) + 
        \alpha y_{n-1} \right].
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">unicornncell(inp, (state, cstate))
unicornncell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the unicornncell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the UnICORNNCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/unicornn_cell.jl#L2-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="RecurrentLayers.WMCLSTMCell" href="#RecurrentLayers.WMCLSTMCell"><code>RecurrentLayers.WMCLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMCLSTMCell(input_size =&gt; hidden_size;
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    init_memory_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/2109.00020">Long short term memory cell with working memory connections</a>. See <a href="../layers/#RecurrentLayers.WMCLSTM"><code>WMCLSTM</code></a> for a layer that processes entire sequences.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_memory_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{i}_t &amp;= \sigma\left(\mathbf{W}_{ix} \mathbf{x}_t + \mathbf{W}_{ih}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{ic} \mathbf{c}_{t-1}) +
        \mathbf{b}_i\right), \\
    \mathbf{f}_t &amp;= \sigma\left(\mathbf{W}_{fx} \mathbf{x}_t + \mathbf{W}_{fh}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{fc} \mathbf{c}_{t-1}) +
        \mathbf{b}_f\right), \\
    \mathbf{o}_t &amp;= \sigma\left(\mathbf{W}_{ox} \mathbf{x}_t + \mathbf{W}_{oh}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{oc} \mathbf{c}_t) + \mathbf{b}_o\right), \\
    \mathbf{c}_t &amp;= \mathbf{f}_t \circ \mathbf{c}_{t-1} + \mathbf{i}_t \circ
        \sigma_c(\mathbf{W}_{c} \mathbf{x}_t + \mathbf{b}_c), \\
    \mathbf{h}_t &amp;= \mathbf{o}_t \circ \sigma_h(\mathbf{c}_t).
\end{aligned}\]</p><p><strong>Forward</strong></p><pre><code class="nohighlight hljs">wmclstmcell(inp, (state, cstate))
wmclstmcell(inp)</code></pre><p><strong>Arguments</strong></p><ul><li><code>inp</code>: The input to the wmclstmcell. It should be a vector of size <code>input_size</code> or a matrix of size <code>input_size x batch_size</code>.</li><li><code>(state, cstate)</code>: A tuple containing the hidden and cell states of the WMCLSTMCell. They should be vectors of size <code>hidden_size</code> or matrices of size <code>hidden_size x batch_size</code>. If not provided, they are assumed to be vectors of zeros, initialized by <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.initialstates"><code>Flux.initialstates</code></a>.</li></ul><p><strong>Returns</strong></p><ul><li>A tuple <code>(output, state)</code>, where <code>output = new_state</code> is the new hidden state and <code>state = (new_state, new_cstate)</code> is the new hidden and cell state.  They are tensors of size <code>hidden_size</code> or <code>hidden_size x batch_size</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/RecurrentLayers.jl/blob/a468a6a372c378206b8c94e2b0f090ff65e55f7e/src/cells/wmclstm_cell.jl#L2-L63">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../layers/">Layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.3 on <span class="colophon-date" title="Thursday 15 May 2025 08:48">Thursday 15 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
